"""
PDQN (Parameterized Deep Q-Network) for CityFlow Multi-Intersection Traffic Signal Control

Designed for a hybrid action space:
- Discrete action: phase selection (per intersection: 0 ~ num_phases-1)
- Continuous parameter: green duration per phase (min_duration ~ max_duration)

PDQN key idea:
1. Q-network: takes state + continuous parameters, outputs Q-values for each discrete action
2. Parameter network: takes state, outputs continuous parameters for each discrete action
3. Action selection: pick the discrete action with highest Q, use its corresponding continuous parameter

Reference:
- Xiong et al. "Parameterized Deep Q-Networks Learning: Reinforcement Learning with 
  Discrete-Continuous Hybrid Action Space" (2018)
"""

import random
import collections
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import os
import sys
from datetime import datetime
from typing import Dict, List, Tuple, Optional

# Add environment path
sys.path.insert(0, os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), "env"))
from cityflow_multi_env import CityFlowMultiIntersectionEnv, get_default_config

# =======================
# Device detection
# =======================
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"[DEVICE] Using device: {DEVICE}", flush=True)
if torch.cuda.is_available():
    print(f"[DEVICE] GPU: {torch.cuda.get_device_name(0)}", flush=True)
    print(f"[DEVICE] CUDA version: {torch.version.cuda}", flush=True)

# =======================
# HYPERPARAMETERS CONFIG
# =======================
CONFIG = {
    # Learning parameters
    'lr_q': 0.001,           # Q-network learning rate
    'lr_param': 0.0003,      # parameter-network learning rate
    'gamma': 0.99,           # discount factor
    'batch_size': 256 if torch.cuda.is_available() else 64,  # larger batch on GPU
    'buffer_limit': 100000,
    'tau': 0.005,            # target network soft update coefficient
    
    # Environment parameters
    'episode_length': 3600,
    'ctrl_interval': 10,
    'min_green': 10,
    'min_duration': 10,
    'max_duration': 60,
    'num_of_episodes': 200,
    
    # Exploration parameters
    'epsilon_start': 1.0,
    'epsilon_end': 0.05,
    'epsilon_decay': 0.995,
    'param_noise_sigma': 0.2,  # exploration noise for continuous parameters
    
    # Network architecture
    'hidden_dim1': 256,
    'hidden_dim2': 128,
    
    # Training parameters
    'memory_threshold': 500,
    'training_iterations': 10,
    'clip_grad_norm': 1.0,
    
    # Output parameters
    'print_interval': 10,
    'save_models': True,
    'output_dir': './outputs/pdqn_cityflow',
}


class ReplayBuffer:
    """Replay buffer for PDQN hybrid actions (GPU-friendly tensors)."""
    def __init__(self, buffer_limit=None, device=None):
        limit = buffer_limit or CONFIG['buffer_limit']
        self.buffer = collections.deque(maxlen=limit)
        self.device = device or DEVICE

    def put(self, transition):
        """
        Store transition: (state, discrete_action, continuous_params, reward, next_state, done)
        """
        self.buffer.append(transition)
        
    def sample(self, n):
        mini_batch = random.sample(self.buffer, n)
        s_lst, disc_a_lst, cont_params_lst, r_lst, s_prime_lst, done_mask_lst = \
            [], [], [], [], [], []

        for transition in mini_batch:
            s, disc_a, cont_params, r, s_prime, done = transition
            s_lst.append(s)
            disc_a_lst.append(disc_a)
            cont_params_lst.append(cont_params)
            r_lst.append(r)
            s_prime_lst.append(s_prime)
            done_mask = 0.0 if done else 1.0 
            done_mask_lst.append([done_mask])
        
        # Create tensors directly on the target device
        return (
            torch.FloatTensor(np.array(s_lst)).to(self.device),
            torch.LongTensor(np.array(disc_a_lst)).to(self.device),
            torch.FloatTensor(np.array(cont_params_lst)).to(self.device),
            torch.FloatTensor(np.array(r_lst)).unsqueeze(1).to(self.device),
            torch.FloatTensor(np.array(s_prime_lst)).to(self.device),
            torch.FloatTensor(np.array(done_mask_lst)).to(self.device)
        )
    
    def size(self):
        return len(self.buffer)


class QNetwork(nn.Module):
    """
    Q ç½‘ç»œï¼šè¾“å…¥çŠ¶æ€å’Œæ‰€æœ‰è¿ç»­å‚æ•°ï¼Œè¾“å‡ºæ¯ä¸ªç¦»æ•£åŠ¨ä½œçš„ Q å€¼
    
    å¯¹äºå¤šè·¯å£åœºæ™¯ï¼š
    - è¾“å…¥: [state, all_continuous_params]
    - è¾“å‡º: Q å€¼å‘é‡ï¼Œç»´åº¦ = num_intersections * num_phases
    
    è¿™é‡Œé‡‡ç”¨åˆ†è§£å¼ Q å€¼ï¼ˆæ¯ä¸ªè·¯å£ç‹¬ç«‹ï¼‰ï¼Œç„¶åèšåˆ
    """
    def __init__(self, state_size: int, num_intersections: int, num_phases: int,
                 hidden_dim1: int = 256, hidden_dim2: int = 128):
        super(QNetwork, self).__init__()
        self.num_intersections = num_intersections
        self.num_phases = num_phases
        
        # è¿ç»­å‚æ•°ç»´åº¦ = num_intersections * num_phases (æ¯ä¸ªåŠ¨ä½œä¸€ä¸ªå‚æ•°)
        self.param_dim = num_intersections * num_phases
        
        # å…±äº«ç‰¹å¾æå–å±‚
        self.fc_shared = nn.Sequential(
            nn.Linear(state_size + self.param_dim, hidden_dim1),
            nn.ReLU(),
            nn.Linear(hidden_dim1, hidden_dim2),
            nn.ReLU()
        )
        
        # æ¯ä¸ªè·¯å£ç‹¬ç«‹çš„ Q å€¼å¤´
        self.q_heads = nn.ModuleList([
            nn.Linear(hidden_dim2, num_phases) for _ in range(num_intersections)
        ])
        
    def forward(self, state: torch.Tensor, continuous_params: torch.Tensor) -> torch.Tensor:
        """
        Args:
            state: (batch, state_dim)
            continuous_params: (batch, num_intersections * num_phases)
        
        Returns:
            q_values: (batch, num_intersections * num_phases) - æ‰€æœ‰è·¯å£æ‰€æœ‰ç›¸ä½çš„ Q å€¼
        """
        # æ‹¼æ¥çŠ¶æ€å’Œè¿ç»­å‚æ•°
        x = torch.cat([state, continuous_params], dim=1)
        
        # å…±äº«ç‰¹å¾
        shared_features = self.fc_shared(x)
        
        # æ¯ä¸ªè·¯å£çš„ Q å€¼
        q_list = []
        for i, q_head in enumerate(self.q_heads):
            q_i = q_head(shared_features)  # (batch, num_phases)
            q_list.append(q_i)
        
        # æ‹¼æ¥æ‰€æœ‰ Q å€¼
        q_values = torch.cat(q_list, dim=1)  # (batch, num_intersections * num_phases)
        
        return q_values
    
    def get_q_per_intersection(self, state: torch.Tensor, continuous_params: torch.Tensor) -> List[torch.Tensor]:
        """è¿”å›æ¯ä¸ªè·¯å£çš„ Q å€¼ï¼ˆç”¨äºåŠ¨ä½œé€‰æ‹©ï¼‰"""
        x = torch.cat([state, continuous_params], dim=1)
        shared_features = self.fc_shared(x)
        
        q_list = []
        for q_head in self.q_heads:
            q_i = q_head(shared_features)
            q_list.append(q_i)
        
        return q_list


class ParameterNetwork(nn.Module):
    """
    å‚æ•°ç½‘ç»œï¼šè¾“å…¥çŠ¶æ€ï¼Œè¾“å‡ºæ‰€æœ‰ç¦»æ•£åŠ¨ä½œå¯¹åº”çš„è¿ç»­å‚æ•°
    
    å¯¹äºå¤šè·¯å£åœºæ™¯ï¼š
    - è¾“å…¥: state
    - è¾“å‡º: æ¯ä¸ªè·¯å£æ¯ä¸ªç›¸ä½çš„ç»¿ç¯æ—¶é•¿å‚æ•° (å½’ä¸€åŒ–åˆ° [0, 1])
    """
    def __init__(self, state_size: int, num_intersections: int, num_phases: int,
                 hidden_dim1: int = 256, hidden_dim2: int = 128):
        super(ParameterNetwork, self).__init__()
        self.num_intersections = num_intersections
        self.num_phases = num_phases
        self.param_dim = num_intersections * num_phases
        
        # å…±äº«ç‰¹å¾æå–
        self.fc_shared = nn.Sequential(
            nn.Linear(state_size, hidden_dim1),
            nn.ReLU(),
            nn.Linear(hidden_dim1, hidden_dim2),
            nn.ReLU()
        )
        
        # å‚æ•°è¾“å‡ºå±‚
        self.fc_params = nn.Linear(hidden_dim2, self.param_dim)
        
    def forward(self, state: torch.Tensor) -> torch.Tensor:
        """
        Args:
            state: (batch, state_dim)
        
        Returns:
            params: (batch, num_intersections * num_phases) - æ¯ä¸ªåŠ¨ä½œçš„è¿ç»­å‚æ•° [0, 1]
        """
        features = self.fc_shared(state)
        params = torch.sigmoid(self.fc_params(features))
        return params


class PDQNAgent:
    """
    PDQN æ™ºèƒ½ä½“ - ç”¨äº CityFlow å¤šè·¯å£äº¤é€šä¿¡å·æ§åˆ¶ (GPU åŠ é€Ÿç‰ˆ)
    
    æ··åˆåŠ¨ä½œç©ºé—´ï¼š
    - ç¦»æ•£åŠ¨ä½œï¼šæ¯ä¸ªè·¯å£é€‰æ‹©ä¸€ä¸ªç›¸ä½
    - è¿ç»­å‚æ•°ï¼šé€‰å®šç›¸ä½çš„ç»¿ç¯æ—¶é•¿
    """
    def __init__(self, env: CityFlowMultiIntersectionEnv, config: Dict = None, device=None):
        self.config = config or CONFIG.copy()
        self.env = env
        self.device = device or DEVICE
        
        # ç¯å¢ƒå‚æ•°
        self.state_size = env.observation_space.shape[0]
        self.num_intersections = env.num_intersections
        self.num_phases = env.num_phases
        self.min_duration = env.min_duration
        self.max_duration = env.max_duration
        self.min_green = env.min_green
        
        # åŠ¨ä½œç©ºé—´ç»´åº¦
        self.discrete_action_size = self.num_intersections  # æ¯ä¸ªè·¯å£ä¸€ä¸ªç¦»æ•£é€‰æ‹©
        self.continuous_param_size = self.num_intersections * self.num_phases  # æ¯ä¸ªåŠ¨ä½œä¸€ä¸ªå‚æ•°
        
        print(f"[PDQN Agent] åˆå§‹åŒ–:")
        print(f"  è®¾å¤‡: {self.device}")
        print(f"  çŠ¶æ€ç»´åº¦: {self.state_size}")
        print(f"  è·¯å£æ•°é‡: {self.num_intersections}")
        print(f"  æ¯ä¸ªè·¯å£ç›¸ä½æ•°: {self.num_phases}")
        print(f"  è¿ç»­å‚æ•°ç»´åº¦: {self.continuous_param_size}")
        print(f"  ç»¿ç¯æ—¶é•¿èŒƒå›´: [{self.min_duration}, {self.max_duration}]ç§’")
        
        # åˆ›å»ºç½‘ç»œ
        hidden1 = self.config['hidden_dim1']
        hidden2 = self.config['hidden_dim2']
        
        # Q ç½‘ç»œ (ç§»åŠ¨åˆ° GPU)
        self.q_network = QNetwork(
            self.state_size, self.num_intersections, self.num_phases, hidden1, hidden2
        ).to(self.device)
        self.q_target = QNetwork(
            self.state_size, self.num_intersections, self.num_phases, hidden1, hidden2
        ).to(self.device)
        self.q_target.load_state_dict(self.q_network.state_dict())
        
        # å‚æ•°ç½‘ç»œ (ç§»åŠ¨åˆ° GPU)
        self.param_network = ParameterNetwork(
            self.state_size, self.num_intersections, self.num_phases, hidden1, hidden2
        ).to(self.device)
        self.param_target = ParameterNetwork(
            self.state_size, self.num_intersections, self.num_phases, hidden1, hidden2
        ).to(self.device)
        self.param_target.load_state_dict(self.param_network.state_dict())
        
        # ä¼˜åŒ–å™¨
        self.q_optimizer = optim.Adam(self.q_network.parameters(), lr=self.config['lr_q'])
        self.param_optimizer = optim.Adam(self.param_network.parameters(), lr=self.config['lr_param'])
        
        # ç»éªŒå›æ”¾ (æ”¯æŒ GPU)
        self.memory = ReplayBuffer(self.config['buffer_limit'], device=self.device)
        
        # æ¢ç´¢å‚æ•°
        self.epsilon = self.config['epsilon_start']
        
    def select_action(self, state: np.ndarray, explore: bool = True) -> Tuple[np.ndarray, np.ndarray]:
        """
        é€‰æ‹©åŠ¨ä½œ
        
        Args:
            state: å½“å‰çŠ¶æ€
            explore: æ˜¯å¦è¿›è¡Œæ¢ç´¢
        
        Returns:
            discrete_actions: æ¯ä¸ªè·¯å£çš„ç›¸ä½é€‰æ‹© (num_intersections,)
            continuous_params: é€‰å®šåŠ¨ä½œçš„ç»¿ç¯æ—¶é•¿ (num_intersections,)
        """
        with torch.no_grad():
            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)
            
            # è·å–æ‰€æœ‰è¿ç»­å‚æ•°
            all_params = self.param_network(state_tensor)  # (1, num_intersections * num_phases)
            
            # æ·»åŠ å‚æ•°å™ªå£°ï¼ˆæ¢ç´¢ï¼‰
            if explore:
                noise = torch.randn_like(all_params) * self.config['param_noise_sigma']
                all_params = torch.clamp(all_params + noise, 0, 1)
            
            # è·å–æ¯ä¸ªè·¯å£çš„ Q å€¼
            q_per_intersection = self.q_network.get_q_per_intersection(state_tensor, all_params)
            
            discrete_actions = []
            continuous_params = []
            
            for i in range(self.num_intersections):
                q_i = q_per_intersection[i].squeeze(0)  # (num_phases,)
                
                # epsilon-greedy é€‰æ‹©
                if explore and random.random() < self.epsilon:
                    action_i = random.randint(0, self.num_phases - 1)
                else:
                    action_i = q_i.argmax().item()
                
                discrete_actions.append(action_i)
                
                # è·å–é€‰å®šåŠ¨ä½œçš„è¿ç»­å‚æ•°
                param_idx = i * self.num_phases + action_i
                param_i = all_params[0, param_idx].item()
                
                # æ˜ å°„åˆ°å®é™…æ—¶é•¿
                duration_i = self.min_duration + param_i * (self.max_duration - self.min_duration)
                continuous_params.append(duration_i)
        
        return np.array(discrete_actions), np.array(continuous_params), all_params.squeeze(0).cpu().numpy()
    
    def convert_to_env_action(self, discrete_actions: np.ndarray, continuous_params: np.ndarray) -> np.ndarray:
        """
        è½¬æ¢ä¸ºç¯å¢ƒåŠ¨ä½œæ ¼å¼
        
        ç¯å¢ƒåŠ¨ä½œæ ¼å¼: [phase_0, duration_idx_0, phase_1, duration_idx_1, ...]
        """
        env_action = np.zeros(self.num_intersections * 2, dtype=np.int64)
        
        for i in range(self.num_intersections):
            env_action[i * 2] = discrete_actions[i]
            
            # å°†è¿ç»­æ—¶é•¿è½¬æ¢ä¸ºæ—¶é•¿ç´¢å¼•
            duration_idx = int(round(continuous_params[i] - self.min_duration))
            duration_idx = np.clip(duration_idx, 0, self.max_duration - self.min_duration)
            env_action[i * 2 + 1] = duration_idx
        
        return env_action
    
    def store_transition(self, state, discrete_actions, all_params, reward, next_state, done):
        """å­˜å‚¨ç»éªŒ"""
        self.memory.put((state, discrete_actions, all_params, reward, next_state, done))
    
    def update(self) -> Tuple[float, float]:
        """
        æ›´æ–°ç½‘ç»œ
        
        Returns:
            q_loss, param_loss
        """
        if self.memory.size() < self.config['batch_size']:
            return 0.0, 0.0
        
        # é‡‡æ ·æ‰¹æ¬¡
        states, disc_actions, cont_params, rewards, next_states, done_masks = \
            self.memory.sample(self.config['batch_size'])
        
        # ========== æ›´æ–° Q ç½‘ç»œ ==========
        # è®¡ç®—ç›®æ ‡ Q å€¼
        with torch.no_grad():
            next_params = self.param_target(next_states)
            next_q_all = self.q_target(next_states, next_params)
            
            # æ¯ä¸ªè·¯å£é€‰æ‹©æœ€å¤§ Q å€¼
            next_q_max = torch.zeros(self.config['batch_size'], 1, device=self.device)
            for i in range(self.num_intersections):
                q_i = next_q_all[:, i * self.num_phases:(i + 1) * self.num_phases]
                next_q_max += q_i.max(dim=1, keepdim=True)[0]
            
            target_q = rewards + self.config['gamma'] * next_q_max * done_masks
        
        # å½“å‰ Q å€¼
        current_q_all = self.q_network(states, cont_params)
        
        # æå–é€‰å®šåŠ¨ä½œçš„ Q å€¼
        current_q = torch.zeros(self.config['batch_size'], 1, device=self.device)
        for i in range(self.num_intersections):
            q_i = current_q_all[:, i * self.num_phases:(i + 1) * self.num_phases]
            action_i = disc_actions[:, i].unsqueeze(1)  # (batch, 1)
            current_q += q_i.gather(1, action_i)
        
        # Q ç½‘ç»œæŸå¤±
        q_loss = F.mse_loss(current_q, target_q)
        
        self.q_optimizer.zero_grad()
        q_loss.backward()
        torch.nn.utils.clip_grad_norm_(self.q_network.parameters(), self.config['clip_grad_norm'])
        self.q_optimizer.step()
        
        # ========== æ›´æ–°å‚æ•°ç½‘ç»œ ==========
        # å‚æ•°ç½‘ç»œçš„ç›®æ ‡ï¼šæœ€å¤§åŒ– Q å€¼
        params = self.param_network(states)
        q_all = self.q_network(states, params)
        
        # è®¡ç®—æ€» Q å€¼ï¼ˆç­–ç•¥æ¢¯åº¦ï¼‰
        param_loss = -q_all.mean()
        
        self.param_optimizer.zero_grad()
        param_loss.backward()
        torch.nn.utils.clip_grad_norm_(self.param_network.parameters(), self.config['clip_grad_norm'])
        self.param_optimizer.step()
        
        return q_loss.item(), param_loss.item()
    
    def soft_update_targets(self):
        """è½¯æ›´æ–°ç›®æ ‡ç½‘ç»œ"""
        tau = self.config['tau']
        
        for target_param, param in zip(self.q_target.parameters(), self.q_network.parameters()):
            target_param.data.copy_(target_param.data * (1 - tau) + param.data * tau)
        
        for target_param, param in zip(self.param_target.parameters(), self.param_network.parameters()):
            target_param.data.copy_(target_param.data * (1 - tau) + param.data * tau)
    
    def decay_epsilon(self):
        """è¡°å‡æ¢ç´¢ç‡"""
        self.epsilon = max(
            self.config['epsilon_end'],
            self.epsilon * self.config['epsilon_decay']
        )
    
    def save(self, path: str):
        """ä¿å­˜æ¨¡å‹"""
        torch.save({
            'q_network': self.q_network.state_dict(),
            'q_target': self.q_target.state_dict(),
            'param_network': self.param_network.state_dict(),
            'param_target': self.param_target.state_dict(),
            'config': self.config,
        }, path)
        print(f"æ¨¡å‹å·²ä¿å­˜: {path}")
    
    def load(self, path: str):
        """åŠ è½½æ¨¡å‹"""
        checkpoint = torch.load(path, map_location=self.device)
        self.q_network.load_state_dict(checkpoint['q_network'])
        self.q_target.load_state_dict(checkpoint['q_target'])
        self.param_network.load_state_dict(checkpoint['param_network'])
        self.param_target.load_state_dict(checkpoint['param_target'])
        print(f"æ¨¡å‹å·²åŠ è½½: {path} (è®¾å¤‡: {self.device})")


class ActionConverter:
    """
    åŠ¨ä½œè½¬æ¢å™¨ - å°† PDQN è¾“å‡ºè½¬æ¢ä¸ºç¯å¢ƒåŠ¨ä½œæ ¼å¼
    
    ä¸åšçº¦æŸæ£€æŸ¥ï¼Œçº¦æŸè¿åç”±ç¯å¢ƒæ£€æµ‹å’Œè®°å½•
    """
    def __init__(self, num_intersections: int, num_phases: int, 
                 min_duration: int, max_duration: int):
        self.num_intersections = num_intersections
        self.num_phases = num_phases
        self.min_duration = min_duration
        self.max_duration = max_duration
    
    def convert(self, discrete_actions: np.ndarray, continuous_params: np.ndarray) -> np.ndarray:
        """
        å°† PDQN è¾“å‡ºè½¬æ¢ä¸ºç¯å¢ƒåŠ¨ä½œæ ¼å¼
        
        ä¸åšçº¦æŸæ£€æŸ¥ï¼Œç›´æ¥è½¬æ¢åŠ¨ä½œæ ¼å¼
        çº¦æŸè¿åç”±ç¯å¢ƒçš„ _apply_action æ£€æµ‹å’Œè®°å½•
        
        Args:
            discrete_actions: æ¯ä¸ªè·¯å£çš„ç›¸ä½é€‰æ‹© (num_intersections,)
            continuous_params: æ¯ä¸ªè·¯å£çš„ç»¿ç¯æ—¶é•¿ (num_intersections,)
        
        Returns:
            env_action: [phase_0, duration_idx_0, phase_1, duration_idx_1, ...]
        """
        env_action = np.zeros(self.num_intersections * 2, dtype=np.int64)
        
        for i in range(self.num_intersections):
            # ç›¸ä½ï¼ˆä»…åšåŸºæœ¬èŒƒå›´è£å‰ªï¼Œä¸æ£€æŸ¥çº¦æŸï¼‰
            phase = int(discrete_actions[i])
            phase = np.clip(phase, 0, self.num_phases - 1)
            env_action[i * 2] = phase
            
            # æ—¶é•¿ç´¢å¼•
            duration_idx = int(round(continuous_params[i] - self.min_duration))
            duration_idx = np.clip(duration_idx, 0, self.max_duration - self.min_duration)
            env_action[i * 2 + 1] = duration_idx
        
        return env_action


def train(config: Dict = None, cityflow_config_path: str = None):
    """è®­ç»ƒ PDQN æ™ºèƒ½ä½“"""
    config = config or CONFIG.copy()
    
    # è®¾ç½® CityFlow é…ç½®è·¯å¾„
    if cityflow_config_path is None:
        script_dir = os.path.dirname(os.path.abspath(__file__))
        cityflow_config_path = os.path.join(script_dir, "../examples/City_3_5/config.json")
    
    # åˆ›å»ºç¯å¢ƒé…ç½®
    env_config = get_default_config(cityflow_config_path)
    env_config["episode_length"] = config['episode_length']
    env_config["ctrl_interval"] = config['ctrl_interval']
    env_config["min_green"] = config['min_green']
    env_config["min_duration"] = config['min_duration']
    env_config["max_duration"] = config['max_duration']
    env_config["verbose_violations"] = False
    env_config["log_violations"] = True
    
    # åˆ›å»ºç¯å¢ƒ
    env = CityFlowMultiIntersectionEnv(env_config)
    
    print(f"\n{'='*60}")
    print("PDQN for CityFlow Traffic Signal Control")
    print(f"{'='*60}")
    
    # åˆ›å»ºæ™ºèƒ½ä½“
    agent = PDQNAgent(env, config)
    
    # åˆ›å»ºåŠ¨ä½œè½¬æ¢å™¨ï¼ˆä¸åšçº¦æŸæ£€æŸ¥ï¼Œçº¦æŸç”±ç¯å¢ƒæ£€æµ‹ï¼‰
    action_converter = ActionConverter(
        env.num_intersections, env.num_phases,
        env.min_duration, env.max_duration
    )
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = config.get('output_dir', './outputs/pdqn_cityflow')
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    run_dir = os.path.join(output_dir, f"run_{timestamp}")
    os.makedirs(run_dir, exist_ok=True)
    
    # è®­ç»ƒè®°å½•
    episode_rewards = []
    episode_travel_times = []
    episode_throughputs = []
    episode_violations = []          # æ€»è¿åæ¬¡æ•°
    episode_violation_details = []   # è¿åè¯¦æƒ…ï¼ˆåˆ†ç±»ï¼‰
    episode_q_losses = []
    episode_param_losses = []
    
    print(f"\nè®­ç»ƒå¼€å§‹ï¼Œè¾“å‡ºç›®å½•: {run_dir}", flush=True)
    print(f"æ€» Episodes: {config['num_of_episodes']}", flush=True)
    print(f"æ¯ Episode æ­¥æ•°: {config['episode_length'] // config['ctrl_interval']}", flush=True)
    print(f"æ‰“å°é—´éš”: æ¯ {config['print_interval']} episodes\n", flush=True)
    
    import time
    train_start_time = time.time()
    
    for n_epi in range(config['num_of_episodes']):
        episode_start_time = time.time()
        state, info = env.reset()
        done = False
        episode_reward = 0
        step = 0
        q_losses, param_losses = [], []
        
        total_steps = config['episode_length'] // config['ctrl_interval']
        
        while not done:
            # é€‰æ‹©åŠ¨ä½œ
            discrete_actions, continuous_params, all_params = agent.select_action(state, explore=True)
            
            # è½¬æ¢ä¸ºç¯å¢ƒåŠ¨ä½œæ ¼å¼ï¼ˆä¸åšçº¦æŸæ£€æŸ¥ï¼‰
            env_action = action_converter.convert(discrete_actions, continuous_params)
            
            # æ‰§è¡ŒåŠ¨ä½œ
            next_state, reward, terminated, truncated, info = env.step(env_action)
            done = terminated or truncated
            
            # å­˜å‚¨ç»éªŒ
            agent.store_transition(state, discrete_actions, all_params, reward, next_state, done)
            
            # æ›´æ–°ç½‘ç»œ
            if agent.memory.size() >= config['memory_threshold']:
                for _ in range(config['training_iterations']):
                    q_loss, param_loss = agent.update()
                    q_losses.append(q_loss)
                    param_losses.append(param_loss)
                
                # è½¯æ›´æ–°ç›®æ ‡ç½‘ç»œ
                agent.soft_update_targets()
            
            episode_reward += reward
            state = next_state
            step += 1
            
            # æ¯100æ­¥æ‰“å°è¿›åº¦ï¼ˆä½¿ç”¨è¿›åº¦æ¡å½¢å¼ï¼‰
            if step % 100 == 0 or done:
                progress = step / total_steps
                bar_len = 20
                filled = int(bar_len * progress)
                bar = 'â–ˆ' * filled + 'â–‘' * (bar_len - filled)
                print(f"\r  [{bar}] {progress*100:5.1f}% | Step {step}/{total_steps} | "
                      f"R={episode_reward:.0f} | Mem={agent.memory.size()}", end="", flush=True)
        
        # è®°å½•ç»Ÿè®¡
        episode_rewards.append(episode_reward)
        avg_travel_time = info.get('average_travel_time', 0)
        episode_travel_times.append(avg_travel_time)
        
        flow_stats = info.get('intersection_flow', {})
        total_throughput = sum(s.get('throughput', 0) for s in flow_stats.values())
        episode_throughputs.append(total_throughput)
        
        # è·å–ç¯å¢ƒç»Ÿè®¡çš„çº¦æŸè¿åï¼ˆç¯å¢ƒæ£€æµ‹å¹¶é˜»æ­¢ï¼‰
        env_violations = info.get('total_violations', {})
        total_viol = sum(env_violations.values()) if env_violations else 0
        episode_violations.append(total_viol)
        episode_violation_details.append(env_violations.copy() if env_violations else {})
        
        avg_q_loss = np.mean(q_losses) if q_losses else 0
        avg_param_loss = np.mean(param_losses) if param_losses else 0
        episode_q_losses.append(avg_q_loss)
        episode_param_losses.append(avg_param_loss)
        
        episode_time = time.time() - episode_start_time
        
        # æ¯ä¸ª Episode ç»“æŸæ‰“å°ç®€è¦ä¿¡æ¯ï¼ˆåŒ…å«æŠ•å½±å™¨é˜»æ­¢çš„è¿åæ¬¡æ•°ï¼‰
        print(f"\nâœ“ Episode {n_epi+1}/{config['num_of_episodes']} å®Œæˆ | "
              f"Reward={episode_reward:.1f} | AvgTT={avg_travel_time:.0f}s | "
              f"Violations={total_viol} | Time={episode_time:.1f}s | Îµ={agent.epsilon:.3f}", flush=True)
        
        # è¡°å‡æ¢ç´¢ç‡
        agent.decay_epsilon()
        
        # æ‰“å°è¯¦ç»†è¿›åº¦ï¼ˆæ¯ print_interval ä¸ª episodeï¼‰
        if (n_epi + 1) % config['print_interval'] == 0:
            avg_reward = np.mean(episode_rewards[-config['print_interval']:])
            avg_tt = np.mean(episode_travel_times[-config['print_interval']:])
            avg_tp = np.mean(episode_throughputs[-config['print_interval']:])
            avg_viol = np.mean(episode_violations[-config['print_interval']:])
            elapsed = time.time() - train_start_time
            
            print(f"\n{'â”€'*60}")
            print(f"ğŸ“Š Episode {n_epi+1}/{config['num_of_episodes']} ç»Ÿè®¡ (è€—æ—¶: {elapsed/60:.1f}åˆ†é’Ÿ)")
            print(f"   å¹³å‡å¥–åŠ±: {avg_reward:.1f}")
            print(f"   å¹³å‡è¡Œç¨‹æ—¶é—´: {avg_tt:.1f}s")
            print(f"   æ€»ååé‡: {avg_tp:.0f}")
            print(f"   çº¦æŸè¿å: {avg_viol:.1f}")
            print(f"   æ¢ç´¢ç‡ Îµ: {agent.epsilon:.3f}")
            print(f"   Q-Loss: {avg_q_loss:.4f}, Param-Loss: {avg_param_loss:.4f}")
            print(f"{'â”€'*60}\n", flush=True)
    
    # ä¿å­˜æ¨¡å‹
    if config.get('save_models', True):
        model_path = os.path.join(run_dir, "pdqn_cityflow_final.pt")
        agent.save(model_path)
    
    # ä¿å­˜è®­ç»ƒæ›²çº¿
    import json
    stats_path = os.path.join(run_dir, "training_stats.json")
    with open(stats_path, 'w') as f:
        json.dump({
            'episode_rewards': episode_rewards,
            'episode_travel_times': episode_travel_times,
            'episode_throughputs': episode_throughputs,
            'episode_violations': episode_violations,
            'episode_violation_details': episode_violation_details,
            'episode_q_losses': episode_q_losses,
            'episode_param_losses': episode_param_losses,
        }, f, indent=2)
    print(f"è®­ç»ƒç»Ÿè®¡å·²ä¿å­˜: {stats_path}")
    
    # æ‰“å°æœ€ç»ˆè¿åç»Ÿè®¡ï¼ˆç¯å¢ƒæ£€æµ‹ï¼‰
    if episode_violations:
        total_min_green = sum(d.get('min_green', 0) for d in episode_violation_details)
        total_target_dur = sum(d.get('target_duration', 0) for d in episode_violation_details)
        total_invalid = sum(d.get('invalid_phase', 0) for d in episode_violation_details)
        print(f"\nçº¦æŸè¿åæ€»è®¡ï¼ˆç¯å¢ƒæ£€æµ‹ï¼‰:")
        print(f"  æœ€å°ç»¿ç¯æ—¶é—´è¿å: {total_min_green}")
        print(f"  ç›®æ ‡æ—¶é•¿è¿å: {total_target_dur}")
        print(f"  æ— æ•ˆç›¸ä½: {total_invalid}")
        print(f"  æ€»è®¡: {sum(episode_violations)}")
    
    env.close()
    
    return {
        'agent': agent,
        'episode_rewards': episode_rewards,
        'episode_travel_times': episode_travel_times,
        'run_dir': run_dir,
    }


def evaluate(model_path: str, cityflow_config_path: str = None, n_episodes: int = 5, render: bool = True):
    """è¯„ä¼°è®­ç»ƒå¥½çš„æ¨¡å‹"""
    # åŠ è½½æ¨¡å‹
    checkpoint = torch.load(model_path)
    config = checkpoint.get('config', CONFIG)
    
    # è®¾ç½® CityFlow é…ç½®è·¯å¾„
    if cityflow_config_path is None:
        script_dir = os.path.dirname(os.path.abspath(__file__))
        cityflow_config_path = os.path.join(script_dir, "../examples/City_3_5/config.json")
    
    # åˆ›å»ºç¯å¢ƒ
    env_config = get_default_config(cityflow_config_path)
    env_config["episode_length"] = config.get('episode_length', 3600)
    env_config["ctrl_interval"] = config.get('ctrl_interval', 10)
    env_config["min_green"] = config.get('min_green', 10)
    env_config["min_duration"] = config.get('min_duration', 10)
    env_config["max_duration"] = config.get('max_duration', 60)
    env_config["verbose_violations"] = render
    
    env = CityFlowMultiIntersectionEnv(env_config, render_mode="human" if render else None)
    
    # åˆ›å»ºæ™ºèƒ½ä½“å¹¶åŠ è½½æƒé‡
    agent = PDQNAgent(env, config)
    agent.load(model_path)
    
    # åˆ›å»ºåŠ¨ä½œè½¬æ¢å™¨ï¼ˆä¸åšçº¦æŸæ£€æŸ¥ï¼Œçº¦æŸç”±ç¯å¢ƒæ£€æµ‹ï¼‰
    action_converter = ActionConverter(
        env.num_intersections, env.num_phases,
        env.min_duration, env.max_duration
    )
    
    print(f"\n{'='*60}")
    print("PDQN æ¨¡å‹è¯„ä¼°")
    print(f"{'='*60}")
    print(f"æ¨¡å‹è·¯å¾„: {model_path}")
    print(f"è¯„ä¼° Episodes: {n_episodes}\n")
    
    episode_rewards = []
    episode_travel_times = []
    episode_violations = []
    
    for ep in range(n_episodes):
        state, info = env.reset()
        done = False
        episode_reward = 0
        
        while not done:
            # é€‰æ‹©åŠ¨ä½œï¼ˆä¸æ¢ç´¢ï¼‰
            discrete_actions, continuous_params, _ = agent.select_action(state, explore=False)
            
            # è½¬æ¢ä¸ºç¯å¢ƒåŠ¨ä½œæ ¼å¼ï¼ˆä¸åšçº¦æŸæ£€æŸ¥ï¼‰
            env_action = action_converter.convert(discrete_actions, continuous_params)
            
            # æ‰§è¡ŒåŠ¨ä½œ
            state, reward, terminated, truncated, info = env.step(env_action)
            done = terminated or truncated
            episode_reward += reward
            
            if render:
                env.render()
        
        episode_rewards.append(episode_reward)
        avg_travel_time = info.get('average_travel_time', 0)
        episode_travel_times.append(avg_travel_time)
        
        # è·å–ç¯å¢ƒè¿åç»Ÿè®¡
        env_violations = info.get('total_violations', {})
        total_viol = sum(env_violations.values()) if env_violations else 0
        episode_violations.append(total_viol)
        
        print(f"Episode {ep+1}/{n_episodes}: "
              f"Reward={episode_reward:.1f}, "
              f"AvgTravelTime={avg_travel_time:.1f}s, "
              f"Violations={total_viol}")
        
        if render:
            env.print_intersection_flow_summary()
            env.print_violation_summary()
    
    env.close()
    
    print(f"\n{'='*60}")
    print("è¯„ä¼°ç»“æœ")
    print(f"{'='*60}")
    print(f"å¹³å‡å¥–åŠ±: {np.mean(episode_rewards):.2f} Â± {np.std(episode_rewards):.2f}")
    print(f"å¹³å‡è¡Œç¨‹æ—¶é—´: {np.mean(episode_travel_times):.2f}s")
    
    return {
        'episode_rewards': episode_rewards,
        'episode_travel_times': episode_travel_times,
    }


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="PDQN for CityFlow Traffic Signal Control")
    parser.add_argument("--mode", type=str, default="train", choices=["train", "evaluate"],
                       help="è¿è¡Œæ¨¡å¼")
    parser.add_argument("--config", type=str, default="../examples/City_3_5/config.json",
                       help="CityFlow é…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--model", type=str, default=None,
                       help="æ¨¡å‹æ–‡ä»¶è·¯å¾„ (evaluate æ¨¡å¼éœ€è¦)")
    parser.add_argument("--episodes", type=int, default=200,
                       help="è®­ç»ƒ episodes æ•°")
    parser.add_argument("--episode-length", type=int, default=3600,
                       help="æ¯ä¸ª episode çš„ä»¿çœŸæ—¶é•¿ï¼ˆç§’ï¼‰")
    parser.add_argument("--min-duration", type=int, default=10,
                       help="æœ€å°ç»¿ç¯æ—¶é•¿ï¼ˆç§’ï¼‰")
    parser.add_argument("--max-duration", type=int, default=60,
                       help="æœ€å¤§ç»¿ç¯æ—¶é•¿ï¼ˆç§’ï¼‰")
    parser.add_argument("--seed", type=int, default=42,
                       help="éšæœºç§å­")
    
    args = parser.parse_args()
    
    # è®¾ç½®éšæœºç§å­
    random.seed(args.seed)
    np.random.seed(args.seed)
    torch.manual_seed(args.seed)
    
    # è·å–è„šæœ¬ç›®å½•
    script_dir = os.path.dirname(os.path.abspath(__file__))
    
    # å¤„ç†é…ç½®æ–‡ä»¶è·¯å¾„
    if not os.path.isabs(args.config):
        config_path = os.path.join(script_dir, args.config)
    else:
        config_path = args.config
    
    if args.mode == "train":
        # æ›´æ–°é…ç½®
        config = CONFIG.copy()
        config['num_of_episodes'] = args.episodes
        config['episode_length'] = args.episode_length
        config['min_duration'] = args.min_duration
        config['max_duration'] = args.max_duration
        
        results = train(config=config, cityflow_config_path=config_path)
        
        print("\nè®­ç»ƒå®Œæˆ!")
        print(f"æœ€ç»ˆå¹³å‡å¥–åŠ±: {np.mean(results['episode_rewards'][-10:]):.2f}")
        print(f"æœ€ç»ˆå¹³å‡è¡Œç¨‹æ—¶é—´: {np.mean(results['episode_travel_times'][-10:]):.2f}s")
        
    elif args.mode == "evaluate":
        if args.model is None:
            print("é”™è¯¯: evaluate æ¨¡å¼éœ€è¦æŒ‡å®š --model å‚æ•°")
            sys.exit(1)
        
        if not os.path.isabs(args.model):
            model_path = os.path.join(script_dir, args.model)
        else:
            model_path = args.model
        
        results = evaluate(model_path, cityflow_config_path=config_path, n_episodes=5)
    
    print("\nå®Œæˆ!")

