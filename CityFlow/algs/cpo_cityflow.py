"""
CPO (Constrained Policy Optimization) for CityFlow Multi-Intersection Traffic Signal Control

CPO æ˜¯ä¸€ç§åŸºäºä¿¡ä»»åŸŸçš„çº¦æŸå¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œèƒ½å¤Ÿåœ¨ä¿è¯çº¦æŸæ»¡è¶³çš„åŒæ—¶ä¼˜åŒ–ç­–ç•¥ã€‚

æ ¸å¿ƒæ€æƒ³ï¼š
1. ä½¿ç”¨ä¿¡ä»»åŸŸæ–¹æ³•ï¼ˆç±»ä¼¼ TRPOï¼‰è¿›è¡Œç­–ç•¥ä¼˜åŒ–
2. åœ¨æ¯æ¬¡æ›´æ–°æ—¶æ±‚è§£çº¦æŸä¼˜åŒ–é—®é¢˜ï¼š
   max_Î¸ E[A(s,a)]
   s.t. KL(Ï€_Î¸ || Ï€_old) â‰¤ Î´  (ä¿¡ä»»åŸŸçº¦æŸ)
        C(Ï€_Î¸) â‰¤ d            (å®‰å…¨çº¦æŸ)

3. ä½¿ç”¨å…±è½­æ¢¯åº¦ + çº¿æœç´¢æ±‚è§£
4. é€šè¿‡æŠ•å½±ç¡®ä¿çº¦æŸæ»¡è¶³

å‚è€ƒè®ºæ–‡ï¼š
- Achiam et al. "Constrained Policy Optimization" (2017)
"""

import random
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.distributions import Categorical, Normal
import os
import sys
from datetime import datetime
from typing import Dict, List, Tuple, Optional
import scipy.optimize

# æ·»åŠ ç¯å¢ƒè·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), "env"))
from cityflow_multi_env import CityFlowMultiIntersectionEnv, get_default_config

# =======================
# GPU è®¾å¤‡æ£€æµ‹
# =======================
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"[DEVICE] ä½¿ç”¨è®¾å¤‡: {DEVICE}", flush=True)
if torch.cuda.is_available():
    print(f"[DEVICE] GPU: {torch.cuda.get_device_name(0)}", flush=True)
    print(f"[DEVICE] CUDA ç‰ˆæœ¬: {torch.version.cuda}", flush=True)

# =======================
# HYPERPARAMETERS CONFIG
# =======================
CONFIG = {
    # CPO parameters
    'lr_critic': 0.001,
    'gamma': 0.99,
    'lambda_gae': 0.95,
    'delta': 0.01,           # KL ä¿¡ä»»åŸŸåŠå¾„
    'cost_limit': 100.0,     # çº¦æŸé˜ˆå€¼ï¼ˆæ¯ episode æœ€å¤§è¿åæ¬¡æ•°ï¼‰
    'cost_gamma': 0.99,
    'damping': 0.1,          # Fisher çŸ©é˜µé˜»å°¼ç³»æ•°
    'max_kl': 0.01,          # æœ€å¤§ KL æ•£åº¦
    'line_search_coef': 0.9, # çº¿æœç´¢è¡°å‡ç³»æ•°
    'line_search_max_iter': 10,
    'cg_iters': 10,          # å…±è½­æ¢¯åº¦è¿­ä»£æ¬¡æ•°
    
    # Value function
    'value_iters': 5,
    
    # Training parameters
    'batch_size': 64,
    'rollout_length': 360,
    'num_of_episodes': 200,
    
    # Environment parameters
    'episode_length': 3600,
    'ctrl_interval': 10,
    'min_green': 10,
    'min_duration': 10,
    'max_duration': 60,
    
    # Network architecture
    'hidden_dim1': 256,
    'hidden_dim2': 128,
    
    # Continuous action parameters
    'log_std_init': -0.5,
    'log_std_min': -2.0,
    'log_std_max': 0.5,
    
    # Output parameters
    'print_interval': 10,
    'save_models': True,
    'output_dir': './outputs/cpo_cityflow',
}


class RolloutBuffer:
    """CPO çš„ Rollout Buffer"""
    def __init__(self):
        self.states = []
        self.discrete_actions = []
        self.continuous_actions = []
        self.rewards = []
        self.costs = []
        self.dones = []
        self.values = []
        self.cost_values = []
        self.disc_log_probs = []
        self.cont_log_probs = []
        
    def store(self, state, disc_action, cont_action, disc_log_prob, cont_log_prob,
              reward, cost, done, value, cost_value):
        self.states.append(state)
        self.discrete_actions.append(disc_action)
        self.continuous_actions.append(cont_action)
        self.disc_log_probs.append(disc_log_prob)
        self.cont_log_probs.append(cont_log_prob)
        self.rewards.append(reward)
        self.costs.append(cost)
        self.dones.append(done)
        self.values.append(value)
        self.cost_values.append(cost_value)
    
    def clear(self):
        self.states = []
        self.discrete_actions = []
        self.continuous_actions = []
        self.rewards = []
        self.costs = []
        self.dones = []
        self.values = []
        self.cost_values = []
        self.disc_log_probs = []
        self.cont_log_probs = []
    
    def compute_gae(self, last_value, last_cost_value, gamma, cost_gamma, lambda_gae):
        """è®¡ç®— GAE ä¼˜åŠ¿"""
        rewards = np.array(self.rewards)
        costs = np.array(self.costs)
        dones = np.array(self.dones)
        values = np.array(self.values + [last_value])
        cost_values = np.array(self.cost_values + [last_cost_value])
        
        # å¥–åŠ± GAE
        advantages = np.zeros_like(rewards)
        last_gae = 0
        for t in reversed(range(len(rewards))):
            next_non_terminal = 1.0 - dones[t]
            delta = rewards[t] + gamma * values[t + 1] * next_non_terminal - values[t]
            advantages[t] = last_gae = delta + gamma * lambda_gae * next_non_terminal * last_gae
        returns = advantages + values[:-1]
        
        # çº¦æŸä»£ä»· GAE
        cost_advantages = np.zeros_like(costs)
        last_cost_gae = 0
        for t in reversed(range(len(costs))):
            next_non_terminal = 1.0 - dones[t]
            delta = costs[t] + cost_gamma * cost_values[t + 1] * next_non_terminal - cost_values[t]
            cost_advantages[t] = last_cost_gae = delta + cost_gamma * lambda_gae * next_non_terminal * last_cost_gae
        cost_returns = cost_advantages + cost_values[:-1]
        
        return returns, advantages, cost_returns, cost_advantages
    
    def get_tensors(self, device):
        """è½¬æ¢ä¸ºå¼ é‡"""
        return {
            'states': torch.FloatTensor(np.array(self.states)).to(device),
            'disc_actions': torch.LongTensor(np.array(self.discrete_actions)).to(device),
            'cont_actions': torch.FloatTensor(np.array(self.continuous_actions)).to(device),
            'disc_log_probs': torch.FloatTensor(np.array(self.disc_log_probs)).to(device),
            'cont_log_probs': torch.FloatTensor(np.array(self.cont_log_probs)).to(device),
        }


class CPOActorCritic(nn.Module):
    """
    CPO Actor-Critic ç½‘ç»œ
    
    Actor: è¾“å‡ºç¦»æ•£å’Œè¿ç»­åŠ¨ä½œçš„åˆ†å¸ƒ
    Reward Critic: ä¼°è®¡å¥–åŠ±ä»·å€¼
    Cost Critic: ä¼°è®¡çº¦æŸä»£ä»·ä»·å€¼
    """
    def __init__(self, state_size: int, num_intersections: int, num_phases: int,
                 hidden_dim1: int = 256, hidden_dim2: int = 128,
                 log_std_init: float = -0.5, log_std_min: float = -2.0, log_std_max: float = 0.5):
        super(CPOActorCritic, self).__init__()
        
        self.num_intersections = num_intersections
        self.num_phases = num_phases
        self.log_std_min = log_std_min
        self.log_std_max = log_std_max
        
        # ========== Actor ==========
        self.actor_shared = nn.Sequential(
            nn.Linear(state_size, hidden_dim1),
            nn.Tanh(),
            nn.Linear(hidden_dim1, hidden_dim2),
            nn.Tanh()
        )
        
        # ç¦»æ•£åŠ¨ä½œå¤´
        self.discrete_heads = nn.ModuleList([
            nn.Linear(hidden_dim2, num_phases) for _ in range(num_intersections)
        ])
        
        # è¿ç»­åŠ¨ä½œå‚æ•°
        self.continuous_mean = nn.Linear(hidden_dim2, num_intersections)
        self.log_std = nn.Parameter(torch.ones(num_intersections) * log_std_init)
        
        # ========== Reward Critic ==========
        self.reward_critic = nn.Sequential(
            nn.Linear(state_size, hidden_dim1),
            nn.Tanh(),
            nn.Linear(hidden_dim1, hidden_dim2),
            nn.Tanh(),
            nn.Linear(hidden_dim2, 1)
        )
        
        # ========== Cost Critic ==========
        self.cost_critic = nn.Sequential(
            nn.Linear(state_size, hidden_dim1),
            nn.Tanh(),
            nn.Linear(hidden_dim1, hidden_dim2),
            nn.Tanh(),
            nn.Linear(hidden_dim2, 1)
        )
    
    def forward_actor(self, state):
        """Actor å‰å‘ä¼ æ’­"""
        actor_features = self.actor_shared(state)
        
        # ç¦»æ•£åŠ¨ä½œ logits
        discrete_logits = [head(actor_features) for head in self.discrete_heads]
        
        # è¿ç»­åŠ¨ä½œå‚æ•°
        cont_mean = torch.sigmoid(self.continuous_mean(actor_features))
        cont_log_std = torch.clamp(self.log_std, self.log_std_min, self.log_std_max)
        cont_std = cont_log_std.exp()
        
        return discrete_logits, cont_mean, cont_std
    
    def forward_critics(self, state):
        """Critic å‰å‘ä¼ æ’­"""
        reward_value = self.reward_critic(state)
        cost_value = self.cost_critic(state)
        return reward_value.squeeze(-1), cost_value.squeeze(-1)
    
    def get_action(self, state, deterministic=False):
        """è·å–åŠ¨ä½œ"""
        discrete_logits, cont_mean, cont_std = self.forward_actor(state)
        
        # ç¦»æ•£åŠ¨ä½œ
        disc_actions = []
        disc_log_probs = []
        disc_entropies = []
        
        for logits in discrete_logits:
            dist = Categorical(logits=logits)
            if deterministic:
                action = logits.argmax(dim=-1)
            else:
                action = dist.sample()
            disc_actions.append(action)
            disc_log_probs.append(dist.log_prob(action))
            disc_entropies.append(dist.entropy())
        
        disc_actions = torch.stack(disc_actions, dim=-1)
        disc_log_probs = torch.stack(disc_log_probs, dim=-1).sum(dim=-1)
        disc_entropy = torch.stack(disc_entropies, dim=-1).mean(dim=-1)
        
        # è¿ç»­åŠ¨ä½œ
        cont_dist = Normal(cont_mean, cont_std)
        if deterministic:
            cont_actions = cont_mean
        else:
            cont_actions = cont_dist.sample()
            cont_actions = torch.clamp(cont_actions, 0, 1)
        
        cont_log_probs = cont_dist.log_prob(cont_actions).sum(dim=-1)
        cont_entropy = cont_dist.entropy().mean(dim=-1)
        
        return disc_actions, cont_actions, disc_log_probs, cont_log_probs, disc_entropy + cont_entropy
    
    def evaluate_actions(self, state, disc_actions, cont_actions):
        """è¯„ä¼°ç»™å®šåŠ¨ä½œçš„å¯¹æ•°æ¦‚ç‡"""
        discrete_logits, cont_mean, cont_std = self.forward_actor(state)
        
        # ç¦»æ•£åŠ¨ä½œ
        disc_log_probs = []
        disc_entropies = []
        
        for i, logits in enumerate(discrete_logits):
            dist = Categorical(logits=logits)
            disc_log_probs.append(dist.log_prob(disc_actions[:, i]))
            disc_entropies.append(dist.entropy())
        
        disc_log_probs = torch.stack(disc_log_probs, dim=-1).sum(dim=-1)
        disc_entropy = torch.stack(disc_entropies, dim=-1).mean(dim=-1)
        
        # è¿ç»­åŠ¨ä½œ
        cont_dist = Normal(cont_mean, cont_std)
        cont_log_probs = cont_dist.log_prob(cont_actions).sum(dim=-1)
        cont_entropy = cont_dist.entropy().mean(dim=-1)
        
        return disc_log_probs, cont_log_probs, disc_entropy + cont_entropy
    
    def get_kl_divergence(self, state, old_disc_logits, old_cont_mean, old_cont_std):
        """è®¡ç®—æ–°æ—§ç­–ç•¥çš„ KL æ•£åº¦"""
        new_disc_logits, new_cont_mean, new_cont_std = self.forward_actor(state)
        
        # ç¦»æ•£åŠ¨ä½œ KL
        disc_kl = 0
        for old_logits, new_logits in zip(old_disc_logits, new_disc_logits):
            old_probs = F.softmax(old_logits, dim=-1)
            new_log_probs = F.log_softmax(new_logits, dim=-1)
            old_log_probs = F.log_softmax(old_logits, dim=-1)
            disc_kl += (old_probs * (old_log_probs - new_log_probs)).sum(dim=-1).mean()
        
        # è¿ç»­åŠ¨ä½œ KL (ä¸¤ä¸ªé«˜æ–¯åˆ†å¸ƒ)
        cont_kl = (
            torch.log(new_cont_std / old_cont_std) +
            (old_cont_std.pow(2) + (old_cont_mean - new_cont_mean).pow(2)) / (2 * new_cont_std.pow(2)) - 0.5
        ).sum(dim=-1).mean()
        
        return disc_kl + cont_kl


class CPOAgent:
    """
    CPO æ™ºèƒ½ä½“
    
    ä½¿ç”¨ä¿¡ä»»åŸŸæ–¹æ³•è¿›è¡Œçº¦æŸç­–ç•¥ä¼˜åŒ–
    """
    def __init__(self, env: CityFlowMultiIntersectionEnv, config: Dict = None, device=None):
        self.config = config or CONFIG.copy()
        self.env = env
        self.device = device or DEVICE
        
        self.state_size = env.observation_space.shape[0]
        self.num_intersections = env.num_intersections
        self.num_phases = env.num_phases
        self.min_duration = env.min_duration
        self.max_duration = env.max_duration
        
        print(f"[CPO Agent] åˆå§‹åŒ–:")
        print(f"  è®¾å¤‡: {self.device}")
        print(f"  çŠ¶æ€ç»´åº¦: {self.state_size}")
        print(f"  è·¯å£æ•°é‡: {self.num_intersections}")
        print(f"  æ¯ä¸ªè·¯å£ç›¸ä½æ•°: {self.num_phases}")
        print(f"  ç»¿ç¯æ—¶é•¿èŒƒå›´: [{self.min_duration}, {self.max_duration}]ç§’")
        print(f"  çº¦æŸé˜ˆå€¼ (cost_limit): {self.config['cost_limit']}")
        print(f"  ä¿¡ä»»åŸŸåŠå¾„ (delta): {self.config['delta']}")
        
        # åˆ›å»ºç½‘ç»œ
        self.actor_critic = CPOActorCritic(
            state_size=self.state_size,
            num_intersections=self.num_intersections,
            num_phases=self.num_phases,
            hidden_dim1=self.config['hidden_dim1'],
            hidden_dim2=self.config['hidden_dim2'],
            log_std_init=self.config['log_std_init'],
            log_std_min=self.config['log_std_min'],
            log_std_max=self.config['log_std_max']
        ).to(self.device)
        
        # Critic ä¼˜åŒ–å™¨
        self.critic_optimizer = optim.Adam(
            list(self.actor_critic.reward_critic.parameters()) + 
            list(self.actor_critic.cost_critic.parameters()),
            lr=self.config['lr_critic']
        )
        
        # Buffer
        self.buffer = RolloutBuffer()
        
    def select_action(self, state: np.ndarray, deterministic: bool = False):
        """é€‰æ‹©åŠ¨ä½œ"""
        with torch.no_grad():
            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)
            
            disc_actions, cont_actions, disc_log_prob, cont_log_prob, _ = \
                self.actor_critic.get_action(state_tensor, deterministic)
            
            reward_value, cost_value = self.actor_critic.forward_critics(state_tensor)
            
            return (
                disc_actions.squeeze(0).cpu().numpy(),
                cont_actions.squeeze(0).cpu().numpy(),
                disc_log_prob.item(),
                cont_log_prob.item(),
                reward_value.item(),
                cost_value.item()
            )
    
    def convert_to_env_action(self, discrete_actions: np.ndarray, continuous_actions: np.ndarray) -> np.ndarray:
        """è½¬æ¢ä¸ºç¯å¢ƒåŠ¨ä½œ"""
        env_action = np.zeros(self.num_intersections * 2, dtype=np.int64)
        
        for i in range(self.num_intersections):
            phase = int(discrete_actions[i])
            phase = np.clip(phase, 0, self.num_phases - 1)
            env_action[i * 2] = phase
            
            duration = self.min_duration + continuous_actions[i] * (self.max_duration - self.min_duration)
            duration_idx = int(round(duration - self.min_duration))
            duration_idx = np.clip(duration_idx, 0, self.max_duration - self.min_duration)
            env_action[i * 2 + 1] = duration_idx
        
        return env_action
    
    def store_transition(self, state, disc_action, cont_action, disc_log_prob, cont_log_prob,
                         reward, cost, done, value, cost_value):
        """å­˜å‚¨ç»éªŒ"""
        self.buffer.store(state, disc_action, cont_action, disc_log_prob, cont_log_prob,
                         reward, cost, done, value, cost_value)
    
    def _flat_grad(self, y, x, retain_graph=False, create_graph=False):
        """è®¡ç®—å±•å¹³çš„æ¢¯åº¦"""
        if create_graph:
            retain_graph = True
        
        g = torch.autograd.grad(y, x, retain_graph=retain_graph, create_graph=create_graph)
        g = torch.cat([t.view(-1) for t in g])
        return g
    
    def _hessian_vector_product(self, kl, params, v, damping=0.1):
        """è®¡ç®— Hessian-å‘é‡ç§¯: H @ v"""
        kl_grad = self._flat_grad(kl, params, retain_graph=True, create_graph=True)
        kl_grad_v = (kl_grad * v).sum()
        hvp = self._flat_grad(kl_grad_v, params, retain_graph=True)
        return hvp + damping * v
    
    def _conjugate_gradient(self, kl, params, b, nsteps=10, residual_tol=1e-10):
        """å…±è½­æ¢¯åº¦æ³•æ±‚è§£ H @ x = b"""
        x = torch.zeros_like(b)
        r = b.clone()
        p = b.clone()
        rdotr = torch.dot(r, r)
        
        for _ in range(nsteps):
            hvp = self._hessian_vector_product(kl, params, p, self.config['damping'])
            alpha = rdotr / (torch.dot(p, hvp) + 1e-8)
            x += alpha * p
            r -= alpha * hvp
            new_rdotr = torch.dot(r, r)
            
            if new_rdotr < residual_tol:
                break
            
            beta = new_rdotr / rdotr
            p = r + beta * p
            rdotr = new_rdotr
        
        return x
    
    def _set_params(self, params, flat_params):
        """è®¾ç½®ç½‘ç»œå‚æ•°"""
        idx = 0
        for p in params:
            numel = p.numel()
            p.data.copy_(flat_params[idx:idx + numel].view(p.shape))
            idx += numel
    
    def _get_flat_params(self, params):
        """è·å–å±•å¹³çš„å‚æ•°"""
        return torch.cat([p.view(-1) for p in params])
    
    def update(self, episode_total_cost: float) -> Dict[str, float]:
        """
        CPO æ›´æ–°
        
        1. æ›´æ–° Critic
        2. ä½¿ç”¨ä¿¡ä»»åŸŸæ–¹æ³•æ›´æ–° Actor
        """
        # è·å–æœ€åçŠ¶æ€ä»·å€¼
        with torch.no_grad():
            last_state = torch.FloatTensor(self.buffer.states[-1]).unsqueeze(0).to(self.device)
            last_reward_value, last_cost_value = self.actor_critic.forward_critics(last_state)
        
        # è®¡ç®— GAE
        returns, advantages, cost_returns, cost_advantages = self.buffer.compute_gae(
            last_reward_value.item(), last_cost_value.item(),
            self.config['gamma'], self.config['cost_gamma'], self.config['lambda_gae']
        )
        
        # è½¬æ¢ä¸ºå¼ é‡
        data = self.buffer.get_tensors(self.device)
        states = data['states']
        disc_actions = data['disc_actions']
        cont_actions = data['cont_actions']
        old_disc_log_probs = data['disc_log_probs']
        old_cont_log_probs = data['cont_log_probs']
        
        returns_tensor = torch.FloatTensor(returns).to(self.device)
        advantages_tensor = torch.FloatTensor(advantages).to(self.device)
        cost_returns_tensor = torch.FloatTensor(cost_returns).to(self.device)
        cost_advantages_tensor = torch.FloatTensor(cost_advantages).to(self.device)
        
        # æ ‡å‡†åŒ–ä¼˜åŠ¿
        advantages_tensor = (advantages_tensor - advantages_tensor.mean()) / (advantages_tensor.std() + 1e-8)
        
        # ========== æ›´æ–° Critic ==========
        for _ in range(self.config['value_iters']):
            reward_values, cost_values = self.actor_critic.forward_critics(states)
            
            reward_value_loss = F.mse_loss(reward_values, returns_tensor)
            cost_value_loss = F.mse_loss(cost_values, cost_returns_tensor)
            value_loss = reward_value_loss + cost_value_loss
            
            self.critic_optimizer.zero_grad()
            value_loss.backward()
            self.critic_optimizer.step()
        
        # ========== CPO Actor æ›´æ–° ==========
        # ä¿å­˜æ—§ç­–ç•¥å‚æ•°
        with torch.no_grad():
            old_disc_logits, old_cont_mean, old_cont_std = self.actor_critic.forward_actor(states)
            old_disc_logits = [logits.clone() for logits in old_disc_logits]
            old_cont_mean = old_cont_mean.clone()
            old_cont_std = old_cont_std.clone()
        
        # è·å– Actor å‚æ•°
        actor_params = list(self.actor_critic.actor_shared.parameters()) + \
                       list(self.actor_critic.discrete_heads.parameters()) + \
                       list(self.actor_critic.continuous_mean.parameters()) + \
                       [self.actor_critic.log_std]
        
        # è®¡ç®—å¥–åŠ±ç›®æ ‡æ¢¯åº¦
        new_disc_log_probs, new_cont_log_probs, entropy = \
            self.actor_critic.evaluate_actions(states, disc_actions, cont_actions)
        
        ratio = torch.exp((new_disc_log_probs + new_cont_log_probs) - 
                         (old_disc_log_probs + old_cont_log_probs))
        
        # å¥–åŠ±ç›®æ ‡
        reward_objective = (ratio * advantages_tensor).mean()
        reward_grad = self._flat_grad(reward_objective, actor_params, retain_graph=True)
        
        # çº¦æŸç›®æ ‡ï¼ˆæœŸæœ›çº¦æŸä»£ä»·ï¼‰
        cost_objective = (ratio * cost_advantages_tensor).mean()
        cost_grad = self._flat_grad(cost_objective, actor_params, retain_graph=True)
        
        # KL æ•£åº¦
        kl = self.actor_critic.get_kl_divergence(states, old_disc_logits, old_cont_mean, old_cont_std)
        
        # ä½¿ç”¨å…±è½­æ¢¯åº¦è®¡ç®—æœç´¢æ–¹å‘
        # å¯¹äºç®€åŒ–ç‰ˆ CPOï¼Œæˆ‘ä»¬ä½¿ç”¨è¿‘ä¼¼æ–¹æ³•
        search_dir = self._conjugate_gradient(kl, actor_params, reward_grad, self.config['cg_iters'])
        
        # è®¡ç®—æ­¥é•¿
        shs = 0.5 * torch.dot(search_dir, self._hessian_vector_product(kl, actor_params, search_dir, self.config['damping']))
        max_step = torch.sqrt(self.config['delta'] / (shs + 1e-8))
        
        # çº¦æŸè°ƒæ•´
        # å¦‚æœè¿åçº¦æŸï¼Œéœ€è¦è°ƒæ•´æ­¥é•¿æ–¹å‘
        cost_violation = episode_total_cost - self.config['cost_limit']
        
        if cost_violation > 0:
            # çº¦æŸè¢«è¿åï¼Œéœ€è¦åœ¨çº¦æŸæ–¹å‘ä¸ŠæŠ•å½±
            cost_search_dir = self._conjugate_gradient(kl, actor_params, cost_grad, self.config['cg_iters'])
            
            # æ··åˆæœç´¢æ–¹å‘
            alpha = min(1.0, cost_violation / (cost_grad.norm() + 1e-8))
            search_dir = search_dir - alpha * cost_search_dir
        
        # çº¿æœç´¢
        old_params = self._get_flat_params(actor_params)
        expected_improvement = torch.dot(reward_grad, search_dir)
        
        step_frac = 1.0
        for _ in range(self.config['line_search_max_iter']):
            new_params = old_params + step_frac * max_step * search_dir
            self._set_params(actor_params, new_params)
            
            with torch.no_grad():
                new_disc_log_probs, new_cont_log_probs, _ = \
                    self.actor_critic.evaluate_actions(states, disc_actions, cont_actions)
                new_ratio = torch.exp((new_disc_log_probs + new_cont_log_probs) - 
                                     (old_disc_log_probs + old_cont_log_probs))
                new_objective = (new_ratio * advantages_tensor).mean()
                new_kl = self.actor_critic.get_kl_divergence(states, old_disc_logits, old_cont_mean, old_cont_std)
            
            improvement = new_objective - reward_objective
            
            if improvement > 0 and new_kl < self.config['max_kl']:
                break
            
            step_frac *= self.config['line_search_coef']
        else:
            # çº¿æœç´¢å¤±è´¥ï¼Œæ¢å¤æ—§å‚æ•°
            self._set_params(actor_params, old_params)
        
        # æ¸…ç©º buffer
        self.buffer.clear()
        
        return {
            'reward_objective': reward_objective.item(),
            'cost_objective': cost_objective.item(),
            'value_loss': value_loss.item(),
            'kl': kl.item() if hasattr(kl, 'item') else kl,
            'cost_violation': cost_violation,
        }
    
    def save(self, path: str):
        """ä¿å­˜æ¨¡å‹"""
        torch.save({
            'actor_critic': self.actor_critic.state_dict(),
            'critic_optimizer': self.critic_optimizer.state_dict(),
            'config': self.config,
        }, path)
        print(f"æ¨¡å‹å·²ä¿å­˜: {path}")
    
    def load(self, path: str):
        """åŠ è½½æ¨¡å‹"""
        checkpoint = torch.load(path, map_location=self.device)
        self.actor_critic.load_state_dict(checkpoint['actor_critic'])
        self.critic_optimizer.load_state_dict(checkpoint['critic_optimizer'])
        print(f"æ¨¡å‹å·²åŠ è½½: {path} (è®¾å¤‡: {self.device})")


def train(config: Dict = None, cityflow_config_path: str = None):
    """è®­ç»ƒ CPO æ™ºèƒ½ä½“"""
    config = config or CONFIG.copy()
    
    if cityflow_config_path is None:
        script_dir = os.path.dirname(os.path.abspath(__file__))
        cityflow_config_path = os.path.join(script_dir, "../examples/City_3_5/config.json")
    
    env_config = get_default_config(cityflow_config_path)
    env_config["episode_length"] = config['episode_length']
    env_config["ctrl_interval"] = config['ctrl_interval']
    env_config["min_green"] = config['min_green']
    env_config["min_duration"] = config['min_duration']
    env_config["max_duration"] = config['max_duration']
    env_config["verbose_violations"] = False
    env_config["log_violations"] = True
    
    env = CityFlowMultiIntersectionEnv(env_config)
    
    print(f"\n{'='*60}")
    print("CPO for CityFlow Traffic Signal Control")
    print(f"{'='*60}")
    
    agent = CPOAgent(env, config)
    
    output_dir = config.get('output_dir', './outputs/cpo_cityflow')
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    run_dir = os.path.join(output_dir, f"run_{timestamp}")
    os.makedirs(run_dir, exist_ok=True)
    
    # è®­ç»ƒè®°å½•
    episode_rewards = []
    episode_travel_times = []
    episode_violations = []
    episode_kls = []
    
    print(f"\nè®­ç»ƒå¼€å§‹ï¼Œè¾“å‡ºç›®å½•: {run_dir}", flush=True)
    print(f"æ€» Episodes: {config['num_of_episodes']}", flush=True)
    print(f"çº¦æŸé˜ˆå€¼: {config['cost_limit']}", flush=True)
    print(f"ä¿¡ä»»åŸŸåŠå¾„: {config['delta']}", flush=True)
    print(f"æ‰“å°é—´éš”: æ¯ {config['print_interval']} episodes\n", flush=True)
    
    import time
    train_start_time = time.time()
    
    for n_epi in range(config['num_of_episodes']):
        episode_start_time = time.time()
        state, info = env.reset()
        done = False
        episode_reward = 0
        episode_cost = 0
        step = 0
        
        prev_total_violations = 0
        total_steps = config['episode_length'] // config['ctrl_interval']
        
        while not done:
            disc_actions, cont_actions, disc_log_prob, cont_log_prob, value, cost_value = \
                agent.select_action(state)
            
            env_action = agent.convert_to_env_action(disc_actions, cont_actions)
            next_state, reward, terminated, truncated, info = env.step(env_action)
            done = terminated or truncated
            
            # è®¡ç®—æ­¥è¿›ä»£ä»·
            current_violations = sum(info.get('total_violations', {}).values())
            step_cost = current_violations - prev_total_violations
            prev_total_violations = current_violations
            
            agent.store_transition(state, disc_actions, cont_actions, disc_log_prob, cont_log_prob,
                                  reward, step_cost, done, value, cost_value)
            
            episode_reward += reward
            episode_cost += step_cost
            state = next_state
            step += 1
            
            if step % 100 == 0 or done:
                progress = step / total_steps
                bar_len = 20
                filled = int(bar_len * progress)
                bar = 'â–ˆ' * filled + 'â–‘' * (bar_len - filled)
                print(f"\r  [{bar}] {progress*100:5.1f}% | Step {step}/{total_steps} | "
                      f"R={episode_reward:.0f} | C={episode_cost:.0f}", end="", flush=True)
        
        # CPO æ›´æ–°
        losses = agent.update(episode_cost)
        
        # ç»Ÿè®¡
        episode_rewards.append(episode_reward)
        avg_travel_time = info.get('average_travel_time', 0)
        episode_travel_times.append(avg_travel_time)
        
        total_viol = sum(info.get('total_violations', {}).values())
        episode_violations.append(total_viol)
        episode_kls.append(losses.get('kl', 0))
        
        episode_time = time.time() - episode_start_time
        
        constraint_status = "âœ“" if total_viol <= config['cost_limit'] else "âœ—"
        
        print(f"\n{constraint_status} Episode {n_epi+1}/{config['num_of_episodes']} å®Œæˆ | "
              f"Reward={episode_reward:.1f} | Cost={total_viol:.0f}/{config['cost_limit']:.0f} | "
              f"KL={losses.get('kl', 0):.4f} | Time={episode_time:.1f}s", flush=True)
        
        if (n_epi + 1) % config['print_interval'] == 0:
            avg_reward = np.mean(episode_rewards[-config['print_interval']:])
            avg_tt = np.mean(episode_travel_times[-config['print_interval']:])
            avg_viol = np.mean(episode_violations[-config['print_interval']:])
            avg_kl = np.mean(episode_kls[-config['print_interval']:])
            elapsed = time.time() - train_start_time
            
            constraint_satisfied = sum(1 for v in episode_violations[-config['print_interval']:] 
                                       if v <= config['cost_limit'])
            satisfaction_rate = constraint_satisfied / config['print_interval'] * 100
            
            print(f"\n{'â”€'*60}")
            print(f"ğŸ“Š Episode {n_epi+1}/{config['num_of_episodes']} ç»Ÿè®¡ (è€—æ—¶: {elapsed/60:.1f}åˆ†é’Ÿ)")
            print(f"   å¹³å‡å¥–åŠ±: {avg_reward:.1f}")
            print(f"   å¹³å‡è¡Œç¨‹æ—¶é—´: {avg_tt:.1f}s")
            print(f"   å¹³å‡çº¦æŸè¿å: {avg_viol:.1f} (é˜ˆå€¼: {config['cost_limit']})")
            print(f"   çº¦æŸæ»¡è¶³ç‡: {satisfaction_rate:.1f}%")
            print(f"   å¹³å‡ KL: {avg_kl:.4f}")
            print(f"{'â”€'*60}\n", flush=True)
    
    # ä¿å­˜
    if config.get('save_models', True):
        model_path = os.path.join(run_dir, "cpo_final.pt")
        agent.save(model_path)
    
    import json
    stats_path = os.path.join(run_dir, "training_stats.json")
    with open(stats_path, 'w') as f:
        json.dump({
            'episode_rewards': episode_rewards,
            'episode_travel_times': episode_travel_times,
            'episode_violations': episode_violations,
            'episode_kls': episode_kls,
        }, f, indent=2)
    print(f"è®­ç»ƒç»Ÿè®¡å·²ä¿å­˜: {stats_path}")
    
    final_satisfaction = sum(1 for v in episode_violations[-20:] if v <= config['cost_limit']) / min(20, len(episode_violations)) * 100
    print(f"\næœ€ç»ˆçº¦æŸæ»¡è¶³ç‡ (å20 episodes): {final_satisfaction:.1f}%")
    
    env.close()
    
    return {
        'agent': agent,
        'episode_rewards': episode_rewards,
        'episode_violations': episode_violations,
        'run_dir': run_dir,
    }


def evaluate(model_path: str, cityflow_config_path: str = None, n_episodes: int = 5, render: bool = True):
    """è¯„ä¼°æ¨¡å‹"""
    checkpoint = torch.load(model_path, map_location=DEVICE)
    config = checkpoint.get('config', CONFIG)
    
    if cityflow_config_path is None:
        script_dir = os.path.dirname(os.path.abspath(__file__))
        cityflow_config_path = os.path.join(script_dir, "../examples/City_3_5/config.json")
    
    env_config = get_default_config(cityflow_config_path)
    env_config["episode_length"] = config.get('episode_length', 3600)
    env_config["ctrl_interval"] = config.get('ctrl_interval', 10)
    env_config["min_green"] = config.get('min_green', 10)
    env_config["min_duration"] = config.get('min_duration', 10)
    env_config["max_duration"] = config.get('max_duration', 60)
    env_config["verbose_violations"] = render
    
    env = CityFlowMultiIntersectionEnv(env_config, render_mode="human" if render else None)
    
    agent = CPOAgent(env, config)
    agent.load(model_path)
    
    print(f"\n{'='*60}")
    print("CPO æ¨¡å‹è¯„ä¼°")
    print(f"{'='*60}")
    print(f"æ¨¡å‹è·¯å¾„: {model_path}")
    print(f"çº¦æŸé˜ˆå€¼: {config.get('cost_limit', 100)}")
    print(f"è¯„ä¼° Episodes: {n_episodes}\n")
    
    episode_rewards = []
    episode_travel_times = []
    episode_violations = []
    
    for ep in range(n_episodes):
        state, info = env.reset()
        done = False
        episode_reward = 0
        
        while not done:
            disc_actions, cont_actions, _, _, _, _ = agent.select_action(state, deterministic=True)
            env_action = agent.convert_to_env_action(disc_actions, cont_actions)
            state, reward, terminated, truncated, info = env.step(env_action)
            done = terminated or truncated
            episode_reward += reward
            
            if render:
                env.render()
        
        episode_rewards.append(episode_reward)
        avg_travel_time = info.get('average_travel_time', 0)
        episode_travel_times.append(avg_travel_time)
        
        total_viol = sum(info.get('total_violations', {}).values())
        episode_violations.append(total_viol)
        
        constraint_status = "âœ“" if total_viol <= config.get('cost_limit', 100) else "âœ—"
        print(f"{constraint_status} Episode {ep+1}/{n_episodes}: "
              f"Reward={episode_reward:.1f}, AvgTT={avg_travel_time:.1f}s, "
              f"Violations={total_viol}")
        
        if render:
            env.print_intersection_flow_summary()
            env.print_violation_summary()
    
    env.close()
    
    satisfaction_rate = sum(1 for v in episode_violations if v <= config.get('cost_limit', 100)) / n_episodes * 100
    
    print(f"\n{'='*60}")
    print("è¯„ä¼°ç»“æœ")
    print(f"{'='*60}")
    print(f"å¹³å‡å¥–åŠ±: {np.mean(episode_rewards):.2f} Â± {np.std(episode_rewards):.2f}")
    print(f"å¹³å‡è¡Œç¨‹æ—¶é—´: {np.mean(episode_travel_times):.2f}s")
    print(f"å¹³å‡çº¦æŸè¿å: {np.mean(episode_violations):.2f}")
    print(f"çº¦æŸæ»¡è¶³ç‡: {satisfaction_rate:.1f}%")
    
    return {
        'episode_rewards': episode_rewards,
        'episode_violations': episode_violations,
        'satisfaction_rate': satisfaction_rate,
    }


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="CPO for CityFlow Traffic Signal Control")
    parser.add_argument("--mode", type=str, default="train", choices=["train", "evaluate"])
    parser.add_argument("--config", type=str, default="../examples/City_3_5/config.json")
    parser.add_argument("--model", type=str, default=None)
    parser.add_argument("--episodes", type=int, default=200)
    parser.add_argument("--episode-length", type=int, default=3600)
    parser.add_argument("--cost-limit", type=float, default=100.0, help="çº¦æŸé˜ˆå€¼")
    parser.add_argument("--delta", type=float, default=0.01, help="ä¿¡ä»»åŸŸåŠå¾„")
    parser.add_argument("--seed", type=int, default=42)
    
    args = parser.parse_args()
    
    random.seed(args.seed)
    np.random.seed(args.seed)
    torch.manual_seed(args.seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(args.seed)
    
    script_dir = os.path.dirname(os.path.abspath(__file__))
    
    if not os.path.isabs(args.config):
        config_path = os.path.join(script_dir, args.config)
    else:
        config_path = args.config
    
    if args.mode == "train":
        config = CONFIG.copy()
        config['num_of_episodes'] = args.episodes
        config['episode_length'] = args.episode_length
        config['cost_limit'] = args.cost_limit
        config['delta'] = args.delta
        
        results = train(config=config, cityflow_config_path=config_path)
        
        print("\nè®­ç»ƒå®Œæˆ!")
        print(f"æœ€ç»ˆå¹³å‡å¥–åŠ±: {np.mean(results['episode_rewards'][-10:]):.2f}")
        print(f"æœ€ç»ˆå¹³å‡çº¦æŸè¿å: {np.mean(results['episode_violations'][-10:]):.2f}")
        
    elif args.mode == "evaluate":
        if args.model is None:
            print("é”™è¯¯: evaluate æ¨¡å¼éœ€è¦æŒ‡å®š --model å‚æ•°")
            sys.exit(1)
        
        if not os.path.isabs(args.model):
            model_path = os.path.join(script_dir, args.model)
        else:
            model_path = args.model
        
        results = evaluate(model_path, cityflow_config_path=config_path, n_episodes=5)
    
    print("\nå®Œæˆ!")

