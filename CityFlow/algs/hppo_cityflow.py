"""
HPPO (Hybrid Proximal Policy Optimization) for CityFlow Multi-Intersection Traffic Signal Control

é€‚ç”¨äºæ··åˆåŠ¨ä½œç©ºé—´ï¼š
- ç¦»æ•£åŠ¨ä½œï¼šç›¸ä½é€‰æ‹© (æ¯ä¸ªè·¯å£é€‰æ‹© 0 ~ num_phases-1)
- è¿ç»­å‚æ•°ï¼šæ¯ä¸ªç›¸ä½å¯¹åº”çš„ç»¿ç¯æ—¶é•¿ (min_duration ~ max_duration)

HPPO æ ¸å¿ƒæ€æƒ³ï¼š
1. Actor ç½‘ç»œåŒæ—¶è¾“å‡ºç¦»æ•£åŠ¨ä½œçš„æ¦‚ç‡åˆ†å¸ƒå’Œè¿ç»­å‚æ•°
2. ä½¿ç”¨ PPO çš„ clipped objective è¿›è¡Œç¨³å®šè®­ç»ƒ
3. ç»“åˆ GAE (Generalized Advantage Estimation) é™ä½æ–¹å·®
4. åŸç”Ÿæ”¯æŒæ··åˆåŠ¨ä½œç©ºé—´

å‚è€ƒè®ºæ–‡ï¼š
- Schulman et al. "Proximal Policy Optimization Algorithms" (2017)
- Fan et al. "Hybrid Actor-Critic Reinforcement Learning in Parameterized Action Space" (2019)
"""

import random
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.distributions import Categorical, Normal
import os
import sys
from datetime import datetime
from typing import Dict, List, Tuple, Optional

# æ·»åŠ ç¯å¢ƒè·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), "env"))
from cityflow_multi_env import CityFlowMultiIntersectionEnv, get_default_config

# =======================
# GPU è®¾å¤‡æ£€æµ‹
# =======================
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"[DEVICE] ä½¿ç”¨è®¾å¤‡: {DEVICE}", flush=True)
if torch.cuda.is_available():
    print(f"[DEVICE] GPU: {torch.cuda.get_device_name(0)}", flush=True)
    print(f"[DEVICE] CUDA ç‰ˆæœ¬: {torch.version.cuda}", flush=True)

# =======================
# HYPERPARAMETERS CONFIG
# =======================
CONFIG = {
    # PPO parameters
    'lr_actor': 0.0003,
    'lr_critic': 0.001,
    'gamma': 0.99,
    'lambda_gae': 0.95,      # GAE lambda
    'epsilon_clip': 0.2,     # PPO clipping parameter
    'entropy_coef': 0.01,    # Entropy bonus coefficient
    'value_coef': 0.5,       # Value loss coefficient
    'max_grad_norm': 0.5,    # Gradient clipping
    
    # Training parameters
    'batch_size': 64,
    'n_epochs': 10,          # PPO epochs per update
    'rollout_length': 360,   # Steps per rollout (1 episode)
    'num_of_episodes': 200,
    
    # Environment parameters
    'episode_length': 3600,
    'ctrl_interval': 10,
    'min_green': 10,
    'min_duration': 10,
    'max_duration': 60,
    
    # Network architecture
    'hidden_dim1': 256,
    'hidden_dim2': 128,
    
    # Continuous action parameters
    'log_std_init': -0.5,    # Initial log std for continuous actions
    'log_std_min': -2.0,
    'log_std_max': 0.5,
    
    # Output parameters
    'print_interval': 10,
    'save_models': True,
    'output_dir': './outputs/hppo_cityflow',
}


class RolloutBuffer:
    """å­˜å‚¨ rollout æ•°æ®ç”¨äº PPO æ›´æ–°"""
    def __init__(self):
        self.states = []
        self.discrete_actions = []
        self.continuous_actions = []
        self.discrete_log_probs = []
        self.continuous_log_probs = []
        self.rewards = []
        self.dones = []
        self.values = []
        
    def store(self, state, disc_action, cont_action, disc_log_prob, cont_log_prob, reward, done, value):
        self.states.append(state)
        self.discrete_actions.append(disc_action)
        self.continuous_actions.append(cont_action)
        self.discrete_log_probs.append(disc_log_prob)
        self.continuous_log_probs.append(cont_log_prob)
        self.rewards.append(reward)
        self.dones.append(done)
        self.values.append(value)
    
    def clear(self):
        self.states = []
        self.discrete_actions = []
        self.continuous_actions = []
        self.discrete_log_probs = []
        self.continuous_log_probs = []
        self.rewards = []
        self.dones = []
        self.values = []
    
    def compute_returns_and_advantages(self, last_value, gamma, lambda_gae):
        """è®¡ç®— GAE ä¼˜åŠ¿å’Œå›æŠ¥"""
        rewards = np.array(self.rewards)
        dones = np.array(self.dones)
        values = np.array(self.values + [last_value])
        
        # GAE
        advantages = np.zeros_like(rewards)
        last_gae = 0
        
        for t in reversed(range(len(rewards))):
            if t == len(rewards) - 1:
                next_non_terminal = 1.0 - dones[t]
                next_value = last_value
            else:
                next_non_terminal = 1.0 - dones[t]
                next_value = values[t + 1]
            
            delta = rewards[t] + gamma * next_value * next_non_terminal - values[t]
            advantages[t] = last_gae = delta + gamma * lambda_gae * next_non_terminal * last_gae
        
        returns = advantages + values[:-1]
        
        return returns, advantages
    
    def get_batches(self, returns, advantages, batch_size, device):
        """ç”Ÿæˆè®­ç»ƒæ‰¹æ¬¡"""
        n_samples = len(self.states)
        indices = np.arange(n_samples)
        np.random.shuffle(indices)
        
        # è½¬æ¢ä¸ºå¼ é‡
        states = torch.FloatTensor(np.array(self.states)).to(device)
        disc_actions = torch.LongTensor(np.array(self.discrete_actions)).to(device)
        cont_actions = torch.FloatTensor(np.array(self.continuous_actions)).to(device)
        disc_log_probs = torch.FloatTensor(np.array(self.discrete_log_probs)).to(device)
        cont_log_probs = torch.FloatTensor(np.array(self.continuous_log_probs)).to(device)
        returns_tensor = torch.FloatTensor(returns).to(device)
        advantages_tensor = torch.FloatTensor(advantages).to(device)
        
        # æ ‡å‡†åŒ–ä¼˜åŠ¿
        advantages_tensor = (advantages_tensor - advantages_tensor.mean()) / (advantages_tensor.std() + 1e-8)
        
        for start in range(0, n_samples, batch_size):
            end = start + batch_size
            batch_indices = indices[start:end]
            
            yield (
                states[batch_indices],
                disc_actions[batch_indices],
                cont_actions[batch_indices],
                disc_log_probs[batch_indices],
                cont_log_probs[batch_indices],
                returns_tensor[batch_indices],
                advantages_tensor[batch_indices]
            )


class HybridActorCritic(nn.Module):
    """
    æ··åˆ Actor-Critic ç½‘ç»œ
    
    Actor è¾“å‡ºï¼š
    - æ¯ä¸ªè·¯å£çš„ç›¸ä½é€‰æ‹©æ¦‚ç‡ (ç¦»æ•£)
    - æ¯ä¸ªè·¯å£çš„ç»¿ç¯æ—¶é•¿å‚æ•° (è¿ç»­ï¼Œå‡å€¼å’Œæ ‡å‡†å·®)
    
    Critic è¾“å‡ºï¼š
    - çŠ¶æ€ä»·å€¼ V(s)
    """
    def __init__(self, state_size: int, num_intersections: int, num_phases: int,
                 hidden_dim1: int = 256, hidden_dim2: int = 128,
                 log_std_init: float = -0.5, log_std_min: float = -2.0, log_std_max: float = 0.5):
        super(HybridActorCritic, self).__init__()
        
        self.num_intersections = num_intersections
        self.num_phases = num_phases
        self.log_std_min = log_std_min
        self.log_std_max = log_std_max
        
        # ========== å…±äº«ç‰¹å¾æå– ==========
        self.shared_fc = nn.Sequential(
            nn.Linear(state_size, hidden_dim1),
            nn.ReLU(),
            nn.Linear(hidden_dim1, hidden_dim2),
            nn.ReLU()
        )
        
        # ========== Actor (ç¦»æ•£åŠ¨ä½œ) ==========
        # æ¯ä¸ªè·¯å£çš„ç›¸ä½é€‰æ‹©æ¦‚ç‡
        self.discrete_heads = nn.ModuleList([
            nn.Linear(hidden_dim2, num_phases) for _ in range(num_intersections)
        ])
        
        # ========== Actor (è¿ç»­åŠ¨ä½œ) ==========
        # æ¯ä¸ªè·¯å£çš„ç»¿ç¯æ—¶é•¿å‚æ•° (å‡å€¼)
        self.continuous_mean = nn.Linear(hidden_dim2, num_intersections)
        
        # å¯å­¦ä¹ çš„ log_std
        self.log_std = nn.Parameter(torch.ones(num_intersections) * log_std_init)
        
        # ========== Critic ==========
        self.critic_fc = nn.Sequential(
            nn.Linear(hidden_dim2, 64),
            nn.ReLU(),
            nn.Linear(64, 1)
        )
        
    def forward(self, state):
        """å‰å‘ä¼ æ’­ï¼Œè¿”å›ç­–ç•¥åˆ†å¸ƒå‚æ•°å’ŒçŠ¶æ€ä»·å€¼"""
        shared_features = self.shared_fc(state)
        
        # ç¦»æ•£åŠ¨ä½œæ¦‚ç‡
        discrete_logits = []
        for head in self.discrete_heads:
            logits = head(shared_features)
            discrete_logits.append(logits)
        
        # è¿ç»­åŠ¨ä½œå‚æ•°
        cont_mean = torch.sigmoid(self.continuous_mean(shared_features))  # [0, 1]
        cont_log_std = torch.clamp(self.log_std, self.log_std_min, self.log_std_max)
        cont_std = cont_log_std.exp()
        
        # çŠ¶æ€ä»·å€¼
        value = self.critic_fc(shared_features)
        
        return discrete_logits, cont_mean, cont_std, value
    
    def get_action_and_value(self, state, discrete_actions=None, continuous_actions=None):
        """
        è·å–åŠ¨ä½œã€å¯¹æ•°æ¦‚ç‡å’Œä»·å€¼
        
        å¦‚æœæä¾›äº†åŠ¨ä½œï¼Œåˆ™è®¡ç®—ç»™å®šåŠ¨ä½œçš„å¯¹æ•°æ¦‚ç‡ï¼ˆç”¨äºè®­ç»ƒï¼‰
        å¦åˆ™é‡‡æ ·æ–°åŠ¨ä½œï¼ˆç”¨äºæ”¶é›†æ•°æ®ï¼‰
        """
        discrete_logits, cont_mean, cont_std, value = self.forward(state)
        
        # ç¦»æ•£åŠ¨ä½œ
        disc_log_probs = []
        disc_entropies = []
        sampled_disc_actions = []
        
        for i, logits in enumerate(discrete_logits):
            dist = Categorical(logits=logits)
            
            if discrete_actions is None:
                action = dist.sample()
            else:
                action = discrete_actions[:, i]
            
            log_prob = dist.log_prob(action)
            entropy = dist.entropy()
            
            sampled_disc_actions.append(action)
            disc_log_probs.append(log_prob)
            disc_entropies.append(entropy)
        
        disc_actions_tensor = torch.stack(sampled_disc_actions, dim=1)
        disc_log_probs_tensor = torch.stack(disc_log_probs, dim=1).sum(dim=1)  # æ‰€æœ‰è·¯å£çš„å¯¹æ•°æ¦‚ç‡ä¹‹å’Œ
        disc_entropy = torch.stack(disc_entropies, dim=1).mean(dim=1)
        
        # è¿ç»­åŠ¨ä½œ
        cont_dist = Normal(cont_mean, cont_std)
        
        if continuous_actions is None:
            cont_actions_tensor = cont_dist.sample()
            cont_actions_tensor = torch.clamp(cont_actions_tensor, 0, 1)
        else:
            cont_actions_tensor = continuous_actions
        
        cont_log_probs_tensor = cont_dist.log_prob(cont_actions_tensor).sum(dim=1)
        cont_entropy = cont_dist.entropy().mean(dim=1)
        
        # æ€»ç†µ
        total_entropy = disc_entropy + cont_entropy
        
        return (disc_actions_tensor, cont_actions_tensor, 
                disc_log_probs_tensor, cont_log_probs_tensor,
                value.squeeze(-1), total_entropy)
    
    def get_value(self, state):
        """ä»…è·å–çŠ¶æ€ä»·å€¼"""
        shared_features = self.shared_fc(state)
        value = self.critic_fc(shared_features)
        return value.squeeze(-1)


class HPPOAgent:
    """
    HPPO æ™ºèƒ½ä½“ - ç”¨äº CityFlow å¤šè·¯å£äº¤é€šä¿¡å·æ§åˆ¶
    """
    def __init__(self, env: CityFlowMultiIntersectionEnv, config: Dict = None, device=None):
        self.config = config or CONFIG.copy()
        self.env = env
        self.device = device or DEVICE
        
        # ç¯å¢ƒå‚æ•°
        self.state_size = env.observation_space.shape[0]
        self.num_intersections = env.num_intersections
        self.num_phases = env.num_phases
        self.min_duration = env.min_duration
        self.max_duration = env.max_duration
        
        print(f"[HPPO Agent] åˆå§‹åŒ–:")
        print(f"  è®¾å¤‡: {self.device}")
        print(f"  çŠ¶æ€ç»´åº¦: {self.state_size}")
        print(f"  è·¯å£æ•°é‡: {self.num_intersections}")
        print(f"  æ¯ä¸ªè·¯å£ç›¸ä½æ•°: {self.num_phases}")
        print(f"  ç»¿ç¯æ—¶é•¿èŒƒå›´: [{self.min_duration}, {self.max_duration}]ç§’")
        
        # åˆ›å»ºç½‘ç»œ
        self.actor_critic = HybridActorCritic(
            state_size=self.state_size,
            num_intersections=self.num_intersections,
            num_phases=self.num_phases,
            hidden_dim1=self.config['hidden_dim1'],
            hidden_dim2=self.config['hidden_dim2'],
            log_std_init=self.config['log_std_init'],
            log_std_min=self.config['log_std_min'],
            log_std_max=self.config['log_std_max']
        ).to(self.device)
        
        # ä¼˜åŒ–å™¨
        self.optimizer = optim.Adam([
            {'params': self.actor_critic.shared_fc.parameters(), 'lr': self.config['lr_actor']},
            {'params': self.actor_critic.discrete_heads.parameters(), 'lr': self.config['lr_actor']},
            {'params': self.actor_critic.continuous_mean.parameters(), 'lr': self.config['lr_actor']},
            {'params': [self.actor_critic.log_std], 'lr': self.config['lr_actor']},
            {'params': self.actor_critic.critic_fc.parameters(), 'lr': self.config['lr_critic']},
        ])
        
        # Rollout buffer
        self.buffer = RolloutBuffer()
        
    def select_action(self, state: np.ndarray) -> Tuple[np.ndarray, np.ndarray, float, float, float]:
        """
        é€‰æ‹©åŠ¨ä½œ
        
        Returns:
            discrete_actions: æ¯ä¸ªè·¯å£çš„ç›¸ä½é€‰æ‹©
            continuous_actions: æ¯ä¸ªè·¯å£çš„ç»¿ç¯æ—¶é•¿ (å½’ä¸€åŒ– [0,1])
            disc_log_prob: ç¦»æ•£åŠ¨ä½œçš„å¯¹æ•°æ¦‚ç‡
            cont_log_prob: è¿ç»­åŠ¨ä½œçš„å¯¹æ•°æ¦‚ç‡
            value: çŠ¶æ€ä»·å€¼
        """
        with torch.no_grad():
            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)
            
            disc_actions, cont_actions, disc_log_prob, cont_log_prob, value, _ = \
                self.actor_critic.get_action_and_value(state_tensor)
            
            return (
                disc_actions.squeeze(0).cpu().numpy(),
                cont_actions.squeeze(0).cpu().numpy(),
                disc_log_prob.item(),
                cont_log_prob.item(),
                value.item()
            )
    
    def convert_to_env_action(self, discrete_actions: np.ndarray, continuous_actions: np.ndarray) -> np.ndarray:
        """
        å°†åŠ¨ä½œè½¬æ¢ä¸ºç¯å¢ƒæ ¼å¼
        
        Args:
            discrete_actions: ç›¸ä½é€‰æ‹© (num_intersections,)
            continuous_actions: å½’ä¸€åŒ–çš„ç»¿ç¯æ—¶é•¿ [0,1] (num_intersections,)
        
        Returns:
            env_action: [phase_0, duration_idx_0, ...]
        """
        env_action = np.zeros(self.num_intersections * 2, dtype=np.int64)
        
        for i in range(self.num_intersections):
            # ç›¸ä½
            phase = int(discrete_actions[i])
            phase = np.clip(phase, 0, self.num_phases - 1)
            env_action[i * 2] = phase
            
            # æ—¶é•¿ï¼šä» [0,1] æ˜ å°„åˆ° [min_duration, max_duration]
            duration = self.min_duration + continuous_actions[i] * (self.max_duration - self.min_duration)
            duration_idx = int(round(duration - self.min_duration))
            duration_idx = np.clip(duration_idx, 0, self.max_duration - self.min_duration)
            env_action[i * 2 + 1] = duration_idx
        
        return env_action
    
    def store_transition(self, state, disc_action, cont_action, disc_log_prob, cont_log_prob, reward, done, value):
        """å­˜å‚¨è½¬æ¢"""
        self.buffer.store(state, disc_action, cont_action, disc_log_prob, cont_log_prob, reward, done, value)
    
    def update(self) -> Dict[str, float]:
        """
        PPO æ›´æ–°
        
        Returns:
            losses: å„é¡¹æŸå¤±çš„å­—å…¸
        """
        # è®¡ç®—æœ€åä¸€ä¸ªçŠ¶æ€çš„ä»·å€¼
        with torch.no_grad():
            last_state = torch.FloatTensor(self.buffer.states[-1]).unsqueeze(0).to(self.device)
            last_value = self.actor_critic.get_value(last_state).item()
        
        # è®¡ç®—å›æŠ¥å’Œä¼˜åŠ¿
        returns, advantages = self.buffer.compute_returns_and_advantages(
            last_value, self.config['gamma'], self.config['lambda_gae']
        )
        
        # PPO å¤šè½®æ›´æ–°
        total_policy_loss = 0
        total_value_loss = 0
        total_entropy_loss = 0
        n_updates = 0
        
        for epoch in range(self.config['n_epochs']):
            for batch in self.buffer.get_batches(returns, advantages, self.config['batch_size'], self.device):
                states, disc_actions, cont_actions, old_disc_log_probs, old_cont_log_probs, batch_returns, batch_advantages = batch
                
                # è·å–å½“å‰ç­–ç•¥çš„åŠ¨ä½œæ¦‚ç‡å’Œä»·å€¼
                _, _, new_disc_log_probs, new_cont_log_probs, values, entropy = \
                    self.actor_critic.get_action_and_value(states, disc_actions, cont_actions)
                
                # è®¡ç®—æ¯”ç‡
                disc_ratio = torch.exp(new_disc_log_probs - old_disc_log_probs)
                cont_ratio = torch.exp(new_cont_log_probs - old_cont_log_probs)
                ratio = disc_ratio * cont_ratio  # æ··åˆæ¯”ç‡
                
                # Clipped surrogate objective
                surr1 = ratio * batch_advantages
                surr2 = torch.clamp(ratio, 1 - self.config['epsilon_clip'], 1 + self.config['epsilon_clip']) * batch_advantages
                policy_loss = -torch.min(surr1, surr2).mean()
                
                # Value loss
                value_loss = F.mse_loss(values, batch_returns)
                
                # Entropy bonus
                entropy_loss = -entropy.mean()
                
                # Total loss
                loss = (policy_loss + 
                       self.config['value_coef'] * value_loss + 
                       self.config['entropy_coef'] * entropy_loss)
                
                # æ›´æ–°
                self.optimizer.zero_grad()
                loss.backward()
                torch.nn.utils.clip_grad_norm_(self.actor_critic.parameters(), self.config['max_grad_norm'])
                self.optimizer.step()
                
                total_policy_loss += policy_loss.item()
                total_value_loss += value_loss.item()
                total_entropy_loss += entropy_loss.item()
                n_updates += 1
        
        # æ¸…ç©º buffer
        self.buffer.clear()
        
        return {
            'policy_loss': total_policy_loss / n_updates,
            'value_loss': total_value_loss / n_updates,
            'entropy_loss': total_entropy_loss / n_updates,
        }
    
    def save(self, path: str):
        """ä¿å­˜æ¨¡å‹"""
        torch.save({
            'actor_critic': self.actor_critic.state_dict(),
            'optimizer': self.optimizer.state_dict(),
            'config': self.config,
        }, path)
        print(f"æ¨¡å‹å·²ä¿å­˜: {path}")
    
    def load(self, path: str):
        """åŠ è½½æ¨¡å‹"""
        checkpoint = torch.load(path, map_location=self.device)
        self.actor_critic.load_state_dict(checkpoint['actor_critic'])
        self.optimizer.load_state_dict(checkpoint['optimizer'])
        print(f"æ¨¡å‹å·²åŠ è½½: {path} (è®¾å¤‡: {self.device})")


def train(config: Dict = None, cityflow_config_path: str = None):
    """è®­ç»ƒ HPPO æ™ºèƒ½ä½“"""
    config = config or CONFIG.copy()
    
    # è®¾ç½® CityFlow é…ç½®è·¯å¾„
    if cityflow_config_path is None:
        script_dir = os.path.dirname(os.path.abspath(__file__))
        cityflow_config_path = os.path.join(script_dir, "../examples/City_3_5/config.json")
    
    # åˆ›å»ºç¯å¢ƒé…ç½®
    env_config = get_default_config(cityflow_config_path)
    env_config["episode_length"] = config['episode_length']
    env_config["ctrl_interval"] = config['ctrl_interval']
    env_config["min_green"] = config['min_green']
    env_config["min_duration"] = config['min_duration']
    env_config["max_duration"] = config['max_duration']
    env_config["verbose_violations"] = False
    env_config["log_violations"] = True
    
    # åˆ›å»ºç¯å¢ƒ
    env = CityFlowMultiIntersectionEnv(env_config)
    
    print(f"\n{'='*60}")
    print("HPPO for CityFlow Traffic Signal Control")
    print(f"{'='*60}")
    
    # åˆ›å»ºæ™ºèƒ½ä½“
    agent = HPPOAgent(env, config)
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = config.get('output_dir', './outputs/hppo_cityflow')
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    run_dir = os.path.join(output_dir, f"run_{timestamp}")
    os.makedirs(run_dir, exist_ok=True)
    
    # è®­ç»ƒè®°å½•
    episode_rewards = []
    episode_travel_times = []
    episode_throughputs = []
    episode_violations = []
    episode_policy_losses = []
    episode_value_losses = []
    
    print(f"\nè®­ç»ƒå¼€å§‹ï¼Œè¾“å‡ºç›®å½•: {run_dir}", flush=True)
    print(f"æ€» Episodes: {config['num_of_episodes']}", flush=True)
    print(f"æ¯ Episode æ­¥æ•°: {config['episode_length'] // config['ctrl_interval']}", flush=True)
    print(f"PPO epochs: {config['n_epochs']}", flush=True)
    print(f"æ‰“å°é—´éš”: æ¯ {config['print_interval']} episodes\n", flush=True)
    
    import time
    train_start_time = time.time()
    
    for n_epi in range(config['num_of_episodes']):
        episode_start_time = time.time()
        state, info = env.reset()
        done = False
        episode_reward = 0
        step = 0
        
        total_steps = config['episode_length'] // config['ctrl_interval']
        
        while not done:
            # é€‰æ‹©åŠ¨ä½œ
            disc_actions, cont_actions, disc_log_prob, cont_log_prob, value = agent.select_action(state)
            
            # è½¬æ¢ä¸ºç¯å¢ƒåŠ¨ä½œ
            env_action = agent.convert_to_env_action(disc_actions, cont_actions)
            
            # æ‰§è¡ŒåŠ¨ä½œ
            next_state, reward, terminated, truncated, info = env.step(env_action)
            done = terminated or truncated
            
            # å­˜å‚¨ç»éªŒ
            agent.store_transition(state, disc_actions, cont_actions, disc_log_prob, cont_log_prob, reward, done, value)
            
            episode_reward += reward
            state = next_state
            step += 1
            
            # æ¯100æ­¥æ‰“å°è¿›åº¦
            if step % 100 == 0 or done:
                progress = step / total_steps
                bar_len = 20
                filled = int(bar_len * progress)
                bar = 'â–ˆ' * filled + 'â–‘' * (bar_len - filled)
                print(f"\r  [{bar}] {progress*100:5.1f}% | Step {step}/{total_steps} | "
                      f"R={episode_reward:.0f}", end="", flush=True)
        
        # PPO æ›´æ–°
        losses = agent.update()
        
        # è®°å½•ç»Ÿè®¡
        episode_rewards.append(episode_reward)
        avg_travel_time = info.get('average_travel_time', 0)
        episode_travel_times.append(avg_travel_time)
        
        flow_stats = info.get('intersection_flow', {})
        total_throughput = sum(s.get('throughput', 0) for s in flow_stats.values())
        episode_throughputs.append(total_throughput)
        
        env_violations = info.get('total_violations', {})
        total_viol = sum(env_violations.values()) if env_violations else 0
        episode_violations.append(total_viol)
        
        episode_policy_losses.append(losses['policy_loss'])
        episode_value_losses.append(losses['value_loss'])
        
        episode_time = time.time() - episode_start_time
        
        # æ‰“å° Episode ç»“æœ
        print(f"\nâœ“ Episode {n_epi+1}/{config['num_of_episodes']} å®Œæˆ | "
              f"Reward={episode_reward:.1f} | AvgTT={avg_travel_time:.0f}s | "
              f"Violations={total_viol} | Time={episode_time:.1f}s", flush=True)
        
        # è¯¦ç»†ç»Ÿè®¡
        if (n_epi + 1) % config['print_interval'] == 0:
            avg_reward = np.mean(episode_rewards[-config['print_interval']:])
            avg_tt = np.mean(episode_travel_times[-config['print_interval']:])
            avg_tp = np.mean(episode_throughputs[-config['print_interval']:])
            avg_viol = np.mean(episode_violations[-config['print_interval']:])
            avg_policy_loss = np.mean(episode_policy_losses[-config['print_interval']:])
            avg_value_loss = np.mean(episode_value_losses[-config['print_interval']:])
            elapsed = time.time() - train_start_time
            
            print(f"\n{'â”€'*60}")
            print(f"ğŸ“Š Episode {n_epi+1}/{config['num_of_episodes']} ç»Ÿè®¡ (è€—æ—¶: {elapsed/60:.1f}åˆ†é’Ÿ)")
            print(f"   å¹³å‡å¥–åŠ±: {avg_reward:.1f}")
            print(f"   å¹³å‡è¡Œç¨‹æ—¶é—´: {avg_tt:.1f}s")
            print(f"   æ€»ååé‡: {avg_tp:.0f}")
            print(f"   çº¦æŸè¿å: {avg_viol:.1f}")
            print(f"   Policy Loss: {avg_policy_loss:.4f}, Value Loss: {avg_value_loss:.4f}")
            print(f"{'â”€'*60}\n", flush=True)
    
    # ä¿å­˜æ¨¡å‹
    if config.get('save_models', True):
        model_path = os.path.join(run_dir, "hppo_cityflow_final.pt")
        agent.save(model_path)
    
    # ä¿å­˜è®­ç»ƒæ›²çº¿
    import json
    stats_path = os.path.join(run_dir, "training_stats.json")
    with open(stats_path, 'w') as f:
        json.dump({
            'episode_rewards': episode_rewards,
            'episode_travel_times': episode_travel_times,
            'episode_throughputs': episode_throughputs,
            'episode_violations': episode_violations,
            'episode_policy_losses': episode_policy_losses,
            'episode_value_losses': episode_value_losses,
        }, f, indent=2)
    print(f"è®­ç»ƒç»Ÿè®¡å·²ä¿å­˜: {stats_path}")
    
    # æ‰“å°æœ€ç»ˆè¿åç»Ÿè®¡
    if episode_violations:
        print(f"\nçº¦æŸè¿åæ€»è®¡ï¼ˆç¯å¢ƒæ£€æµ‹ï¼‰: {sum(episode_violations)}")
    
    env.close()
    
    return {
        'agent': agent,
        'episode_rewards': episode_rewards,
        'episode_travel_times': episode_travel_times,
        'run_dir': run_dir,
    }


def evaluate(model_path: str, cityflow_config_path: str = None, n_episodes: int = 5, render: bool = True):
    """è¯„ä¼°è®­ç»ƒå¥½çš„æ¨¡å‹"""
    # åŠ è½½æ¨¡å‹
    checkpoint = torch.load(model_path, map_location=DEVICE)
    config = checkpoint.get('config', CONFIG)
    
    # è®¾ç½® CityFlow é…ç½®è·¯å¾„
    if cityflow_config_path is None:
        script_dir = os.path.dirname(os.path.abspath(__file__))
        cityflow_config_path = os.path.join(script_dir, "../examples/City_3_5/config.json")
    
    # åˆ›å»ºç¯å¢ƒ
    env_config = get_default_config(cityflow_config_path)
    env_config["episode_length"] = config.get('episode_length', 3600)
    env_config["ctrl_interval"] = config.get('ctrl_interval', 10)
    env_config["min_green"] = config.get('min_green', 10)
    env_config["min_duration"] = config.get('min_duration', 10)
    env_config["max_duration"] = config.get('max_duration', 60)
    env_config["verbose_violations"] = render
    
    env = CityFlowMultiIntersectionEnv(env_config, render_mode="human" if render else None)
    
    # åˆ›å»ºæ™ºèƒ½ä½“å¹¶åŠ è½½æƒé‡
    agent = HPPOAgent(env, config)
    agent.load(model_path)
    
    print(f"\n{'='*60}")
    print("HPPO æ¨¡å‹è¯„ä¼°")
    print(f"{'='*60}")
    print(f"æ¨¡å‹è·¯å¾„: {model_path}")
    print(f"è¯„ä¼° Episodes: {n_episodes}\n")
    
    episode_rewards = []
    episode_travel_times = []
    episode_violations = []
    
    for ep in range(n_episodes):
        state, info = env.reset()
        done = False
        episode_reward = 0
        
        while not done:
            # é€‰æ‹©åŠ¨ä½œï¼ˆè¯„ä¼°æ—¶ä¸éœ€è¦å­˜å‚¨ï¼‰
            disc_actions, cont_actions, _, _, _ = agent.select_action(state)
            
            # è½¬æ¢ä¸ºç¯å¢ƒåŠ¨ä½œ
            env_action = agent.convert_to_env_action(disc_actions, cont_actions)
            
            # æ‰§è¡ŒåŠ¨ä½œ
            state, reward, terminated, truncated, info = env.step(env_action)
            done = terminated or truncated
            episode_reward += reward
            
            if render:
                env.render()
        
        episode_rewards.append(episode_reward)
        avg_travel_time = info.get('average_travel_time', 0)
        episode_travel_times.append(avg_travel_time)
        
        env_violations = info.get('total_violations', {})
        total_viol = sum(env_violations.values()) if env_violations else 0
        episode_violations.append(total_viol)
        
        print(f"Episode {ep+1}/{n_episodes}: "
              f"Reward={episode_reward:.1f}, "
              f"AvgTravelTime={avg_travel_time:.1f}s, "
              f"Violations={total_viol}")
        
        if render:
            env.print_intersection_flow_summary()
            env.print_violation_summary()
    
    env.close()
    
    print(f"\n{'='*60}")
    print("è¯„ä¼°ç»“æœ")
    print(f"{'='*60}")
    print(f"å¹³å‡å¥–åŠ±: {np.mean(episode_rewards):.2f} Â± {np.std(episode_rewards):.2f}")
    print(f"å¹³å‡è¡Œç¨‹æ—¶é—´: {np.mean(episode_travel_times):.2f}s")
    print(f"å¹³å‡çº¦æŸè¿å: {np.mean(episode_violations):.2f}")
    
    return {
        'episode_rewards': episode_rewards,
        'episode_travel_times': episode_travel_times,
        'episode_violations': episode_violations,
    }


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="HPPO for CityFlow Traffic Signal Control")
    parser.add_argument("--mode", type=str, default="train", choices=["train", "evaluate"],
                       help="è¿è¡Œæ¨¡å¼")
    parser.add_argument("--config", type=str, default="../examples/City_3_5/config.json",
                       help="CityFlow é…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--model", type=str, default=None,
                       help="æ¨¡å‹æ–‡ä»¶è·¯å¾„ (evaluate æ¨¡å¼éœ€è¦)")
    parser.add_argument("--episodes", type=int, default=200,
                       help="è®­ç»ƒ episodes æ•°")
    parser.add_argument("--episode-length", type=int, default=3600,
                       help="æ¯ä¸ª episode çš„ä»¿çœŸæ—¶é•¿ï¼ˆç§’ï¼‰")
    parser.add_argument("--min-duration", type=int, default=10,
                       help="æœ€å°ç»¿ç¯æ—¶é•¿ï¼ˆç§’ï¼‰")
    parser.add_argument("--max-duration", type=int, default=60,
                       help="æœ€å¤§ç»¿ç¯æ—¶é•¿ï¼ˆç§’ï¼‰")
    parser.add_argument("--seed", type=int, default=42,
                       help="éšæœºç§å­")
    
    args = parser.parse_args()
    
    # è®¾ç½®éšæœºç§å­
    random.seed(args.seed)
    np.random.seed(args.seed)
    torch.manual_seed(args.seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(args.seed)
    
    # è·å–è„šæœ¬ç›®å½•
    script_dir = os.path.dirname(os.path.abspath(__file__))
    
    # å¤„ç†é…ç½®æ–‡ä»¶è·¯å¾„
    if not os.path.isabs(args.config):
        config_path = os.path.join(script_dir, args.config)
    else:
        config_path = args.config
    
    if args.mode == "train":
        # æ›´æ–°é…ç½®
        config = CONFIG.copy()
        config['num_of_episodes'] = args.episodes
        config['episode_length'] = args.episode_length
        config['min_duration'] = args.min_duration
        config['max_duration'] = args.max_duration
        
        results = train(config=config, cityflow_config_path=config_path)
        
        print("\nè®­ç»ƒå®Œæˆ!")
        print(f"æœ€ç»ˆå¹³å‡å¥–åŠ±: {np.mean(results['episode_rewards'][-10:]):.2f}")
        print(f"æœ€ç»ˆå¹³å‡è¡Œç¨‹æ—¶é—´: {np.mean(results['episode_travel_times'][-10:]):.2f}s")
        
    elif args.mode == "evaluate":
        if args.model is None:
            print("é”™è¯¯: evaluate æ¨¡å¼éœ€è¦æŒ‡å®š --model å‚æ•°")
            sys.exit(1)
        
        if not os.path.isabs(args.model):
            model_path = os.path.join(script_dir, args.model)
        else:
            model_path = args.model
        
        results = evaluate(model_path, cityflow_config_path=config_path, n_episodes=5)
    
    print("\nå®Œæˆ!")

